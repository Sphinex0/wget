<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Gradient descent - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"e919514d-2b15-4c76-b8d9-1f8ce7136741","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Gradient_descent","wgTitle":"Gradient descent","wgCurRevisionId":1300672032,"wgRevisionId":1300672032,"wgArticleId":201489,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1:Vancouver names with accept markup","Webarchive template wayback links","Articles with short description","Short description matches Wikidata","Commons category link is on Wikidata","Mathematical optimization","First order methods","Optimization algorithms and methods","Gradient methods"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Gradient_descent","wgRelevantArticleId":201489,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgCiteReferencePreviewsActive":false,"wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":0,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":40000,"wgMetricsPlatformUserExperiments":{"active_experiments":[],"overrides":[],"enrolled":[],"assigned":[],"subject_ids":[],"sampling_units":[]},"wgEditSubmitButtonLabelPublish":true,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q1199743","wgCheckUserClientHintsHeadersJsApi":["brands","architecture","bitness","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGESuggestedEditsTaskTypes":{"taskTypes":["copyedit","link-recommendation"],"unavailableTaskTypes":[]},"wgGETopicsMatchModeEnabled":false,"wgGELevelingUpEnabledForUser":false};
RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","ext.tmh.player.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.wikimediamessages.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.xLab","ext.cite.ux-enhancements","mediawiki.page.media","ext.tmh.player","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.bootstrap","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","ext.checkUser.clientHints","ext.quicksurveys.init","ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.tmh.player.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediamessages.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.45.0-wmf.12">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Gradient_Descent_in_2D.webm/720px--Gradient_Descent_in_2D.webm.jpg">
<meta property="og:image:width" content="720">
<meta property="og:image:height" content="720">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Gradient_Descent_in_2D.webm/720px--Gradient_Descent_in_2D.webm.jpg">
<meta property="og:image:width" content="720">
<meta property="og:image:height" content="720">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Gradient_Descent_in_2D.webm/640px--Gradient_Descent_in_2D.webm.jpg">
<meta property="og:image:width" content="640">
<meta property="og:image:height" content="640">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="Gradient descent - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" media="only screen and (max-width: 640px)" href="//en.m.wikipedia.org/wiki/Gradient_descent">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Gradient_descent&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Gradient_descent">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="auth.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Gradient_descent rootpage-Gradient_descent skin-vector-2022 action-view"><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header no-font-mode-scale">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  title="Main menu" >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li>
		</ul>
		
	</div>
</div>

	
	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li><li id="n-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages"><span>Special pages</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input mw-searchInput" autocomplete="off"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" spellcheck="false" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  title="Change the appearance of the page&#039;s font size, width, and color" >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-sitesupport-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en" class=""><span>Donate</span></a>
</li>
<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:CreateAccount&amp;returnto=Gradient+descent" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:UserLogin&amp;returnto=Gradient+descent" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-sitesupport" class="user-links-collapsible-item mw-list-item"><a href="https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en"><span>Donate</span></a></li><li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Gradient+descent" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Gradient+descent" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-user-menu-anon-editor" class="vector-menu mw-portlet mw-portlet-user-menu-anon-editor"  >
	<div class="vector-menu-heading">
		Pages for logged out editors <a href="/wiki/Help:Introduction" aria-label="Learn more about editing"><span>learn more</span></a>
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Description"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Description">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>Description</span>
			</div>
		</a>
		
			<button aria-controls="toc-Description-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Description subsection</span>
			</button>
		
		<ul id="toc-Description-sublist" class="vector-toc-list">
			<li id="toc-An_analogy_for_understanding_gradient_descent"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#An_analogy_for_understanding_gradient_descent">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">1.1</span>
					<span>An analogy for understanding gradient descent</span>
				</div>
			</a>
			
			<ul id="toc-An_analogy_for_understanding_gradient_descent-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Choosing_the_step_size_and_descent_direction"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Choosing_the_step_size_and_descent_direction">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">1.2</span>
					<span>Choosing the step size and descent direction</span>
				</div>
			</a>
			
			<ul id="toc-Choosing_the_step_size_and_descent_direction-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Solution_of_a_linear_system"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Solution_of_a_linear_system">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>Solution of a linear system</span>
			</div>
		</a>
		
			<button aria-controls="toc-Solution_of_a_linear_system-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Solution of a linear system subsection</span>
			</button>
		
		<ul id="toc-Solution_of_a_linear_system-sublist" class="vector-toc-list">
			<li id="toc-Geometric_behavior_and_residual_orthogonality"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Geometric_behavior_and_residual_orthogonality">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">2.1</span>
					<span>Geometric behavior and residual orthogonality</span>
				</div>
			</a>
			
			<ul id="toc-Geometric_behavior_and_residual_orthogonality-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Solution_of_a_non-linear_system"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Solution_of_a_non-linear_system">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>Solution of a non-linear system</span>
			</div>
		</a>
		
		<ul id="toc-Solution_of_a_non-linear_system-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Comments"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Comments">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>Comments</span>
			</div>
		</a>
		
		<ul id="toc-Comments-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Modifications"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Modifications">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>Modifications</span>
			</div>
		</a>
		
			<button aria-controls="toc-Modifications-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Modifications subsection</span>
			</button>
		
		<ul id="toc-Modifications-sublist" class="vector-toc-list">
			<li id="toc-Fast_gradient_methods"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Fast_gradient_methods">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">5.1</span>
					<span>Fast gradient methods</span>
				</div>
			</a>
			
			<ul id="toc-Fast_gradient_methods-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Momentum_or_heavy_ball_method"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Momentum_or_heavy_ball_method">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">5.2</span>
					<span>Momentum or <i>heavy ball</i> method</span>
				</div>
			</a>
			
			<ul id="toc-Momentum_or_heavy_ball_method-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Extensions"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Extensions">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>Extensions</span>
			</div>
		</a>
		
		<ul id="toc-Extensions-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Theoretical_properties"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Theoretical_properties">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>Theoretical properties</span>
			</div>
		</a>
		
		<ul id="toc-Theoretical_properties-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">8</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">9</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Further_reading"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Further_reading">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">10</span>
				<span>Further reading</span>
			</div>
		</a>
		
		<ul id="toc-Further_reading-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-External_links"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#External_links">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">11</span>
				<span>External links</span>
			</div>
		</a>
		
		<ul id="toc-External_links-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar no-font-mode-scale">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  title="Table of Contents" >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Gradient descent</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 26 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-26" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">26 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%AE%D9%88%D8%A7%D8%B1%D8%B2%D9%85%D9%8A%D8%A9_%D8%A3%D8%B5%D9%84_%D8%A7%D9%84%D8%AA%D8%AF%D8%B1%D8%AC" title="خوارزمية أصل التدرج – Arabic" lang="ar" hreflang="ar" data-title="خوارزمية أصل التدرج" data-language-autonym="العربية" data-language-local-name="Arabic" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-bg mw-list-item"><a href="https://bg.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0%D0%BD%D0%B5" title="Градиентно спускане – Bulgarian" lang="bg" hreflang="bg" data-title="Градиентно спускане" data-language-autonym="Български" data-language-local-name="Bulgarian" class="interlanguage-link-target"><span>Български</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Algorisme_del_gradient_descendent" title="Algorisme del gradient descendent – Catalan" lang="ca" hreflang="ca" data-title="Algorisme del gradient descendent" data-language-autonym="Català" data-language-local-name="Catalan" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-cs mw-list-item"><a href="https://cs.wikipedia.org/wiki/Gradientn%C3%AD_sestup" title="Gradientní sestup – Czech" lang="cs" hreflang="cs" data-title="Gradientní sestup" data-language-autonym="Čeština" data-language-local-name="Czech" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/Gradientenverfahren" title="Gradientenverfahren – German" lang="de" hreflang="de" data-title="Gradientenverfahren" data-language-autonym="Deutsch" data-language-local-name="German" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Descenso_del_gradiente" title="Descenso del gradiente – Spanish" lang="es" hreflang="es" data-title="Descenso del gradiente" data-language-autonym="Español" data-language-local-name="Spanish" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%DA%AF%D8%B1%D8%A7%D8%AF%DB%8C%D8%A7%D9%86_%DA%A9%D8%A7%D9%87%D8%B4%DB%8C" title="گرادیان کاهشی – Persian" lang="fa" hreflang="fa" data-title="گرادیان کاهشی" data-language-autonym="فارسی" data-language-local-name="Persian" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Algorithme_du_gradient" title="Algorithme du gradient – French" lang="fr" hreflang="fr" data-title="Algorithme du gradient" data-language-autonym="Français" data-language-local-name="French" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95" title="경사 하강법 – Korean" lang="ko" hreflang="ko" data-title="경사 하강법" data-language-autonym="한국어" data-language-local-name="Korean" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-id mw-list-item"><a href="https://id.wikipedia.org/wiki/Penurunan_gradien" title="Penurunan gradien – Indonesian" lang="id" hreflang="id" data-title="Penurunan gradien" data-language-autonym="Bahasa Indonesia" data-language-local-name="Indonesian" class="interlanguage-link-target"><span>Bahasa Indonesia</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Discesa_del_gradiente" title="Discesa del gradiente – Italian" lang="it" hreflang="it" data-title="Discesa del gradiente" data-language-autonym="Italiano" data-language-local-name="Italian" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/Gradient_descent" title="Gradient descent – Hebrew" lang="he" hreflang="he" data-title="Gradient descent" data-language-autonym="עברית" data-language-local-name="Hebrew" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-lt mw-list-item"><a href="https://lt.wikipedia.org/wiki/Gradientinis_nusileidimas" title="Gradientinis nusileidimas – Lithuanian" lang="lt" hreflang="lt" data-title="Gradientinis nusileidimas" data-language-autonym="Lietuvių" data-language-local-name="Lithuanian" class="interlanguage-link-target"><span>Lietuvių</span></a></li><li class="interlanguage-link interwiki-ml mw-list-item"><a href="https://ml.wikipedia.org/wiki/%E0%B4%97%E0%B5%8D%E0%B4%B0%E0%B5%87%E0%B4%A1%E0%B4%BF%E0%B4%AF%E0%B4%A8%E0%B5%8D%E0%B4%B1%E0%B5%8D_%E0%B4%A1%E0%B4%BF%E0%B4%B8%E0%B5%86%E0%B4%A8%E0%B5%8D%E0%B4%B1%E0%B5%8D" title="ഗ്രേഡിയന്റ് ഡിസെന്റ് – Malayalam" lang="ml" hreflang="ml" data-title="ഗ്രേഡിയന്റ് ഡിസെന്റ്" data-language-autonym="മലയാളം" data-language-local-name="Malayalam" class="interlanguage-link-target"><span>മലയാളം</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E6%9C%80%E6%80%A5%E9%99%8D%E4%B8%8B%E6%B3%95" title="最急降下法 – Japanese" lang="ja" hreflang="ja" data-title="最急降下法" data-language-autonym="日本語" data-language-local-name="Japanese" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-pl mw-list-item"><a href="https://pl.wikipedia.org/wiki/Metoda_gradientu_prostego" title="Metoda gradientu prostego – Polish" lang="pl" hreflang="pl" data-title="Metoda gradientu prostego" data-language-autonym="Polski" data-language-local-name="Polish" class="interlanguage-link-target"><span>Polski</span></a></li><li class="interlanguage-link interwiki-pt mw-list-item"><a href="https://pt.wikipedia.org/wiki/M%C3%A9todo_do_gradiente" title="Método do gradiente – Portuguese" lang="pt" hreflang="pt" data-title="Método do gradiente" data-language-autonym="Português" data-language-local-name="Portuguese" class="interlanguage-link-target"><span>Português</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA" title="Градиентный спуск – Russian" lang="ru" hreflang="ru" data-title="Градиентный спуск" data-language-autonym="Русский" data-language-local-name="Russian" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%B0%D0%BC_%D0%BE%D0%BF%D0%B0%D0%B4%D0%B0%D1%98%D1%83%D1%9B%D0%B5%D0%B3_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D1%98%D0%B5%D0%BD%D1%82%D0%B0" title="Алгоритам опадајућег градијента – Serbian" lang="sr" hreflang="sr" data-title="Алгоритам опадајућег градијента" data-language-autonym="Српски / srpski" data-language-local-name="Serbian" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-sv mw-list-item"><a href="https://sv.wikipedia.org/wiki/Gradientnedstigning" title="Gradientnedstigning – Swedish" lang="sv" hreflang="sv" data-title="Gradientnedstigning" data-language-autonym="Svenska" data-language-local-name="Swedish" class="interlanguage-link-target"><span>Svenska</span></a></li><li class="interlanguage-link interwiki-th mw-list-item"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%84%E0%B8%A5%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B8%A5%E0%B8%87%E0%B8%95%E0%B8%B2%E0%B8%A1%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%8A%E0%B8%B1%E0%B8%99" title="การเคลื่อนลงตามความชัน – Thai" lang="th" hreflang="th" data-title="การเคลื่อนลงตามความชัน" data-language-autonym="ไทย" data-language-local-name="Thai" class="interlanguage-link-target"><span>ไทย</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D1%96%D1%94%D0%BD%D1%82%D0%BD%D0%B8%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA" title="Градієнтний спуск – Ukrainian" lang="uk" hreflang="uk" data-title="Градієнтний спуск" data-language-autonym="Українська" data-language-local-name="Ukrainian" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/Suy_gi%E1%BA%A3m_%C4%91%E1%BB%99_d%E1%BB%91c" title="Suy giảm độ dốc – Vietnamese" lang="vi" hreflang="vi" data-title="Suy giảm độ dốc" data-language-autonym="Tiếng Việt" data-language-local-name="Vietnamese" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-wuu mw-list-item"><a href="https://wuu.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" title="梯度下降法 – Wu" lang="wuu" hreflang="wuu" data-title="梯度下降法" data-language-autonym="吴语" data-language-local-name="Wu" class="interlanguage-link-target"><span>吴语</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" title="梯度下降法 – Cantonese" lang="yue" hreflang="yue" data-title="梯度下降法" data-language-autonym="粵語" data-language-local-name="Cantonese" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" title="梯度下降法 – Chinese" lang="zh" hreflang="zh" data-title="梯度下降法" data-language-autonym="中文" data-language-local-name="Chinese" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q1199743#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar vector-feature-custom-font-size-clientpref--excluded">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Gradient_descent" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Gradient_descent" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Gradient_descent"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Gradient_descent"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Gradient_descent" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Gradient_descent" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;oldid=1300672032" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Gradient_descent&amp;id=1300672032&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGradient_descent"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGradient_descent"><span>Download QR code</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Gradient_descent&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Gradient_descent&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-wikibase-otherprojects" class="vector-menu mw-portlet mw-portlet-wikibase-otherprojects"  >
	<div class="vector-menu-heading">
		In other projects
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li class="wb-otherproject-link wb-otherproject-commons mw-list-item"><a href="https://commons.wikimedia.org/wiki/Category:Gradient_descent" hreflang="en"><span>Wikimedia Commons</span></a></li><li id="t-wikibase" class="wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q1199743" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end no-font-mode-scale">
					<div class="vector-sticky-pinned-container">
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Optimization algorithm</div>
<style data-mw-deduplicate="TemplateStyles:r1236090951">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role="note" class="hatnote navigation-not-searchable">For the analytical method called "steepest descent", see <a href="/wiki/Method_of_steepest_descent" title="Method of steepest descent">Method of steepest descent</a>.</div>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1246091330">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}}</style><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047488"><table class="sidebar sidebar-collapse nomobile nowraplinks"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br>and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Paradigms</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" class="mw-redirect" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" title="Meta-learning (computer science)">Meta-learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Batch_learning" class="mw-redirect" title="Batch learning">Batch learning</a></li>
<li><a href="/wiki/Curriculum_learning" title="Curriculum learning">Curriculum learning</a></li>
<li><a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">Rule-based learning</a></li>
<li><a href="/wiki/Neuro-symbolic_AI" title="Neuro-symbolic AI">Neuro-symbolic AI</a></li>
<li><a href="/wiki/Neuromorphic_engineering" class="mw-redirect" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Quantum_machine_learning" title="Quantum machine learning">Quantum machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Generative_model" title="Generative model">Generative modeling</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></li>
<li><a href="/wiki/Density_estimation" title="Density estimation">Density estimation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_cleaning" class="mw-redirect" title="Data cleaning">Data cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Multimodal_learning" title="Multimodal learning">Multimodal learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><div style="display: inline-block; line-height: 1.2em; padding: .1em 0;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br><span class="nobold"><span style="font-size: 85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&nbsp;• <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">Fuzzy</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">SDL</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Random_sample_consensus" title="Random sample consensus">RANSAC</a></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
<li><a href="/wiki/Isolation_forest" title="Isolation forest">Isolation forest</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Neural networks</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward neural network</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li>
<li><a href="/wiki/Reservoir_computing" title="Reservoir computing">reservoir computing</a></li></ul></li>
<li><a href="/wiki/Boltzmann_machine" title="Boltzmann machine">Boltzmann machine</a>
<ul><li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted</a></li></ul></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Diffusion_model" title="Diffusion model">Diffusion model</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li>
<li><a href="/wiki/LeNet" title="LeNet">LeNet</a></li>
<li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li></ul></li>
<li><a href="/wiki/Neural_field" title="Neural field">Neural field</a>
<ul><li><a href="/wiki/Neural_radiance_field" title="Neural radiance field">Neural radiance field</a></li>
<li><a href="/wiki/Physics-informed_neural_networks" title="Physics-informed neural networks">Physics-informed neural networks</a></li></ul></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent</a>
<ul><li><a href="/wiki/Self-play_(reinforcement_learning_technique)" class="mw-redirect" title="Self-play (reinforcement learning technique)">Self-play</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Learning with humans</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a></li>
<li><a href="/wiki/Crowdsourcing" title="Crowdsourcing">Crowdsourcing</a></li>
<li><a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">Human-in-the-loop</a></li>
<li><a href="/wiki/Mechanistic_interpretability" title="Mechanistic interpretability">Mechanistic interpretability</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Model diagnostics</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Coefficient_of_determination" title="Coefficient of determination">Coefficient of determination</a></li>
<li><a href="/wiki/Confusion_matrix" title="Confusion matrix">Confusion matrix</a></li>
<li><a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">Learning curve</a></li>
<li><a href="/wiki/Receiver_operating_characteristic" title="Receiver operating characteristic">ROC curve</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Mathematical foundations</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
<li><a href="/wiki/Topological_deep_learning" title="Topological deep learning">Topological deep learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Journals and conferences</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/AAAI_Conference_on_Artificial_Intelligence" title="AAAI Conference on Artificial Intelligence">AAAI</a></li>
<li><a href="/wiki/ECML_PKDD" title="ECML PKDD">ECML PKDD</a></li>
<li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/International_Conference_on_Learning_Representations" title="International Conference on Learning Representations">ICLR</a></li>
<li><a href="/wiki/International_Joint_Conference_on_Artificial_Intelligence" title="International Joint Conference on Artificial Intelligence">IJCAI</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed machine-learning-list-title"><div class="sidebar-list-title" style="border-top:1px solid #aaa; text-align:center;;color: var(--color-base)">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a>
<ul><li><a href="/wiki/List_of_datasets_in_computer_vision_and_image_processing" title="List of datasets in computer vision and image processing">List of datasets in computer vision and image processing</a></li></ul></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1239400231">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Machine_learning" title="Special:EditPage/Template:Machine learning"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<figure class="mw-default-size mw-halign-right" typeof="mw:File/Thumb"><span><video id="mwe_player_0" poster="//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Gradient_Descent_in_2D.webm/250px--Gradient_Descent_in_2D.webm.jpg" controls="" preload="none" data-mw-tmh="" class="mw-file-element" width="250" height="250" data-durationhint="14" data-mwtitle="Gradient_Descent_in_2D.webm" data-mwprovider="wikimediacommons" resource="/wiki/File:Gradient_Descent_in_2D.webm"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.480p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="480p.vp9.webm" data-width="480" data-height="480"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.720p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="720p.vp9.webm" data-width="720" data-height="720"><source src="//upload.wikimedia.org/wikipedia/commons/4/4c/Gradient_Descent_in_2D.webm" type="video/webm; codecs=&quot;vp8&quot;" data-width="720" data-height="720"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.144p.mjpeg.mov" type="video/quicktime" data-transcodekey="144p.mjpeg.mov" data-width="144" data-height="144"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.240p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="240p.vp9.webm" data-width="240" data-height="240"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.360p.vp9.webm" type="video/webm; codecs=&quot;vp9, opus&quot;" data-transcodekey="360p.vp9.webm" data-width="360" data-height="360"><source src="//upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.360p.webm" type="video/webm; codecs=&quot;vp8, vorbis&quot;" data-transcodekey="360p.webm" data-width="360" data-height="360"></video></span><figcaption>Gradient Descent in 2D</figcaption></figure>
<p><b>Gradient descent</b> is a method for unconstrained <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">mathematical optimization</a>. It is a <a href="/wiki/Category:First_order_methods" title="Category:First order methods">first-order</a> <a href="/wiki/Iterative_algorithm" class="mw-redirect" title="Iterative algorithm">iterative</a> <a href="/wiki/Algorithm" title="Algorithm">algorithm</a> for minimizing a <a href="/wiki/Differentiable_function" title="Differentiable function">differentiable</a> <a href="/wiki/Multivariate_function" class="mw-redirect" title="Multivariate function">multivariate function</a>.
</p><p>The idea is to take repeated steps in the opposite direction of the <a href="/wiki/Gradient" title="Gradient">gradient</a> (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as <i>gradient ascent</i>.
It is particularly useful in machine learning for minimizing the cost or loss function.<sup id="cite_ref-auto_1-0" class="reference"><a href="#cite_note-auto-1"><span class="cite-bracket">[</span>1<span class="cite-bracket">]</span></a></sup> Gradient descent should not be confused with <a href="/wiki/Local_search_(optimization)" title="Local search (optimization)">local search</a> algorithms, although both are <a href="/wiki/Iterative_method" title="Iterative method">iterative methods</a> for <a href="/wiki/Global_optimization" title="Global optimization">optimization</a>.
</p><p>Gradient descent is generally attributed to <a href="/wiki/Augustin-Louis_Cauchy" title="Augustin-Louis Cauchy">Augustin-Louis Cauchy</a>, who first suggested it in 1847.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2"><span class="cite-bracket">[</span>2<span class="cite-bracket">]</span></a></sup> <a href="/wiki/Jacques_Hadamard" title="Jacques Hadamard">Jacques Hadamard</a> independently proposed a similar method in 1907.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span class="cite-bracket">[</span>3<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span class="cite-bracket">[</span>4<span class="cite-bracket">]</span></a></sup> Its convergence properties for non-linear optimization problems were first studied by <a href="/wiki/Haskell_Curry" title="Haskell Curry">Haskell Curry</a> in 1944,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span class="cite-bracket">[</span>5<span class="cite-bracket">]</span></a></sup> with the method becoming increasingly well-studied and used in the following decades.<sup id="cite_ref-BP_6-0" class="reference"><a href="#cite_note-BP-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-AK82_7-0" class="reference"><a href="#cite_note-AK82-7"><span class="cite-bracket">[</span>7<span class="cite-bracket">]</span></a></sup>
</p><p>A simple extension of gradient descent, <a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a>, serves as the most basic algorithm used for training most <a href="/wiki/Deep_neural_network" class="mw-redirect" title="Deep neural network">deep networks</a> today.
</p>
<meta property="mw:PageProp/toc">
<div class="mw-heading mw-heading2"><h2 id="Description">Description</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=1" title="Edit section: Description"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure typeof="mw:File/Thumb"><a href="/wiki/File:Gradient_descent.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/500px-Gradient_descent.svg.png" decoding="async" width="350" height="375" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/525px-Gradient_descent.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/700px-Gradient_descent.svg.png 2x" data-file-width="512" data-file-height="549"></a><figcaption>Illustration of gradient descent on a series of <a href="/wiki/Level_set" title="Level set">level sets</a></figcaption></figure>
<p>Gradient descent is based on the observation that if the <a href="/wiki/Multi-variable_function" class="mw-redirect" title="Multi-variable function">multi-variable function</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {x} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {x} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e41ea95e6949bf4cef6426116364ba87e0fdcd60" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:4.499ex; height:2.843ex;" alt="{\displaystyle f(\mathbf {x} )}"></span> is <a href="/wiki/Defined_and_undefined" class="mw-redirect" title="Defined and undefined">defined</a> and <a href="/wiki/Differentiable_function" title="Differentiable function">differentiable</a> in a neighborhood of a point <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {a} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">a</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {a} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a957216653a9ee0d0133dcefd13fb75e36b8b9d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.299ex; height:1.676ex;" alt="{\displaystyle \mathbf {a} }"></span>, then <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {x} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {x} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e41ea95e6949bf4cef6426116364ba87e0fdcd60" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:4.499ex; height:2.843ex;" alt="{\displaystyle f(\mathbf {x} )}"></span> decreases <i>fastest</i> if one goes from <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {a} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">a</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {a} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a957216653a9ee0d0133dcefd13fb75e36b8b9d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.299ex; height:1.676ex;" alt="{\displaystyle \mathbf {a} }"></span> in the direction of the negative <a href="/wiki/Gradient" title="Gradient">gradient</a> of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> at <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {a} ,-\nabla f(\mathbf {a} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">a</mi>
        </mrow>
        <mo>,</mo>
        <mo>−<!-- − --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">a</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {a} ,-\nabla f(\mathbf {a} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ee3eb66d4725855e644890d4053a8d1ad09de336" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:10.465ex; height:2.843ex;" alt="{\displaystyle \mathbf {a} ,-\nabla f(\mathbf {a} )}"></span>. It follows that, if
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {a} _{n+1}=\mathbf {a} _{n}-\eta \nabla f(\mathbf {a} _{n})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <mi>η<!-- η --></mi>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {a} _{n+1}=\mathbf {a} _{n}-\eta \nabla f(\mathbf {a} _{n})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0da6df8bb221173d4069002b90216ce1e4896fe9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:21.786ex; height:2.843ex;" alt="{\displaystyle \mathbf {a} _{n+1}=\mathbf {a} _{n}-\eta \nabla f(\mathbf {a} _{n})}"></span></dd></dl>
<p>for a small enough step size or <a href="/wiki/Learning_rate" title="Learning rate">learning rate</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta \in \mathbb {R} _{+}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
        <mo>∈<!-- ∈ --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>+</mo>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta \in \mathbb {R} _{+}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6dbcb888d2ec54232c942ac839c75958042f8ca7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:7.199ex; height:2.676ex;" alt="{\displaystyle \eta \in \mathbb {R} _{+}}"></span>, then  <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {a_{n}} )\geq f(\mathbf {a_{n+1}} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">a</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">n</mi>
            </mrow>
          </msub>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">a</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">n</mi>
              <mo mathvariant="bold">+</mo>
              <mn mathvariant="bold">1</mn>
            </mrow>
          </msub>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {a_{n}} )\geq f(\mathbf {a_{n+1}} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f37eed2684f3af5cc064385a8638276c3c90cc0d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:16.852ex; height:2.843ex;" alt="{\displaystyle f(\mathbf {a_{n}} )\geq f(\mathbf {a_{n+1}} )}"></span>. In other words, the term <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta \nabla f(\mathbf {a} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">a</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta \nabla f(\mathbf {a} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b45f2033014e5bb30ea034d03448dda0e32cbc0b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:7.493ex; height:2.843ex;" alt="{\displaystyle \eta \nabla f(\mathbf {a} )}"></span> is subtracted from <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {a} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">a</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {a} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a957216653a9ee0d0133dcefd13fb75e36b8b9d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.299ex; height:1.676ex;" alt="{\displaystyle \mathbf {a} }"></span> because we want to move against the gradient, toward the local minimum. With this observation in mind, one starts with a guess <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} _{0}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} _{0}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/799c59f89751f24a2719c4da95f1acdd3e2faf52" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.465ex; height:2.009ex;" alt="{\displaystyle \mathbf {x} _{0}}"></span> for a local minimum of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span>, and considers the sequence <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} _{0},\mathbf {x} _{1},\mathbf {x} _{2},\ldots }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} _{0},\mathbf {x} _{1},\mathbf {x} _{2},\ldots }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01c74f93321057d30c00a1963150b237313ba874" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:13.221ex; height:2.009ex;" alt="{\displaystyle \mathbf {x} _{0},\mathbf {x} _{1},\mathbf {x} _{2},\ldots }"></span> such that
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} _{n+1}=\mathbf {x} _{n}-\eta _{n}\nabla f(\mathbf {x} _{n}),\ n\geq 0.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mtext>&nbsp;</mtext>
        <mi>n</mi>
        <mo>≥<!-- ≥ --></mo>
        <mn>0.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} _{n+1}=\mathbf {x} _{n}-\eta _{n}\nabla f(\mathbf {x} _{n}),\ n\geq 0.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f80cdf71d71a02e58d4bc3cf81663cb5f760399" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:31.242ex; height:2.843ex;" alt="{\displaystyle \mathbf {x} _{n+1}=\mathbf {x} _{n}-\eta _{n}\nabla f(\mathbf {x} _{n}),\ n\geq 0.}"></span></dd></dl>
<p>We have a <a href="/wiki/Monotonic_function" title="Monotonic function">monotonic</a> sequence
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {x} _{0})\geq f(\mathbf {x} _{1})\geq f(\mathbf {x} _{2})\geq \cdots ,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mo>⋯<!-- ⋯ --></mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {x} _{0})\geq f(\mathbf {x} _{1})\geq f(\mathbf {x} _{2})\geq \cdots ,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/240e43829142583ad0a8fbd0002a3e5345c3aca7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:29.712ex; height:2.843ex;" alt="{\displaystyle f(\mathbf {x} _{0})\geq f(\mathbf {x} _{1})\geq f(\mathbf {x} _{2})\geq \cdots ,}"></span></dd></dl>
<p>so the sequence <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle (\mathbf {x} _{n})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (\mathbf {x} _{n})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/42c4ac385d2fc09aa0390781f96441e9b8615b95" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:4.439ex; height:2.843ex;" alt="{\displaystyle (\mathbf {x} _{n})}"></span> converges to the desired local minimum. Note that the value of the <i>step size</i> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span> is allowed to change at every iteration. 
</p><p>It is possible to guarantee the <a href="/wiki/Convergent_series" title="Convergent series">convergence</a> to a local minimum under certain assumptions on the function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> (for example, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> <a href="/wiki/Convex_function" title="Convex function">convex</a> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7b4d6de89b52c5a5e6e1583cb63eaee263e307b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:3.214ex; height:2.509ex;" alt="{\displaystyle \nabla f}"></span> <a href="/wiki/Lipschitz_continuity" title="Lipschitz continuity">Lipschitz</a>) and particular choices of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span>. Those include the sequence
</p><p><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}={\frac {\left|\left(\mathbf {x} _{n}-\mathbf {x} _{n-1}\right)^{\top }\left[\nabla f(\mathbf {x} _{n})-\nabla f(\mathbf {x} _{n-1})\right]\right|}{\left\|\nabla f(\mathbf {x} _{n})-\nabla f(\mathbf {x} _{n-1})\right\|^{2}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mo>|</mo>
              <mrow>
                <msup>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi mathvariant="bold">x</mi>
                        </mrow>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi>n</mi>
                        </mrow>
                      </msub>
                      <mo>−<!-- − --></mo>
                      <msub>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi mathvariant="bold">x</mi>
                        </mrow>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi>n</mi>
                          <mo>−<!-- − --></mo>
                          <mn>1</mn>
                        </mrow>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                  </mrow>
                </msup>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <mi mathvariant="normal">∇<!-- ∇ --></mi>
                    <mi>f</mi>
                    <mo stretchy="false">(</mo>
                    <msub>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi mathvariant="bold">x</mi>
                      </mrow>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>n</mi>
                      </mrow>
                    </msub>
                    <mo stretchy="false">)</mo>
                    <mo>−<!-- − --></mo>
                    <mi mathvariant="normal">∇<!-- ∇ --></mi>
                    <mi>f</mi>
                    <mo stretchy="false">(</mo>
                    <msub>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi mathvariant="bold">x</mi>
                      </mrow>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>n</mi>
                        <mo>−<!-- − --></mo>
                        <mn>1</mn>
                      </mrow>
                    </msub>
                    <mo stretchy="false">)</mo>
                  </mrow>
                  <mo>]</mo>
                </mrow>
              </mrow>
              <mo>|</mo>
            </mrow>
            <msup>
              <mrow>
                <mo symmetric="true">‖</mo>
                <mrow>
                  <mi mathvariant="normal">∇<!-- ∇ --></mi>
                  <mi>f</mi>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">x</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mo>−<!-- − --></mo>
                  <mi mathvariant="normal">∇<!-- ∇ --></mi>
                  <mi>f</mi>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">x</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                      <mo>−<!-- − --></mo>
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mrow>
                <mo symmetric="true">‖</mo>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}={\frac {\left|\left(\mathbf {x} _{n}-\mathbf {x} _{n-1}\right)^{\top }\left[\nabla f(\mathbf {x} _{n})-\nabla f(\mathbf {x} _{n-1})\right]\right|}{\left\|\nabla f(\mathbf {x} _{n})-\nabla f(\mathbf {x} _{n-1})\right\|^{2}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/80f60d3675a08013f5cf38fcaf1c610eb1d96c46" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:43.05ex; height:7.843ex;" alt="{\displaystyle \eta _{n}={\frac {\left|\left(\mathbf {x} _{n}-\mathbf {x} _{n-1}\right)^{\top }\left[\nabla f(\mathbf {x} _{n})-\nabla f(\mathbf {x} _{n-1})\right]\right|}{\left\|\nabla f(\mathbf {x} _{n})-\nabla f(\mathbf {x} _{n-1})\right\|^{2}}}}"></span>
</p><p>as in the <a href="/wiki/Barzilai-Borwein_method" class="mw-redirect" title="Barzilai-Borwein method">Barzilai-Borwein method</a>,<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span class="cite-bracket">[</span>8<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span class="cite-bracket">[</span>9<span class="cite-bracket">]</span></a></sup> or a sequence <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> satisfying the <a href="/wiki/Wolfe_conditions" title="Wolfe conditions">Wolfe conditions</a> (which can be found by using <a href="/wiki/Line_search" title="Line search">line search</a>). When the function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is <a href="/wiki/Convex_function" title="Convex function">convex</a>, all local minima are also global minima, so in this case gradient descent can converge to the global solution.
</p><p>This process is illustrated in the adjacent picture. Here, <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is assumed to be defined on the plane, and that its graph has a <a href="/wiki/Bowl_(vessel)" class="mw-redirect" title="Bowl (vessel)">bowl</a> shape.  The blue curves are the <a href="/wiki/Contour_line" title="Contour line">contour lines</a>, that is, the regions on which the value of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is constant. A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is <a href="/wiki/Orthogonal" class="mw-redirect" title="Orthogonal">orthogonal</a> to the contour line going through that point. We see that gradient <i>descent</i> leads us to the bottom of the bowl, that is, to the point where the value of the function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is minimal.
</p>
<div class="mw-heading mw-heading3"><h3 id="An_analogy_for_understanding_gradient_descent">An analogy for understanding gradient descent</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=2" title="Edit section: An analogy for understanding gradient descent"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Okanogan-Wenatchee_National_Forest,_morning_fog_shrouds_trees_(37171636495).jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Okanogan-Wenatchee_National_Forest%2C_morning_fog_shrouds_trees_%2837171636495%29.jpg/250px-Okanogan-Wenatchee_National_Forest%2C_morning_fog_shrouds_trees_%2837171636495%29.jpg" decoding="async" width="250" height="166" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Okanogan-Wenatchee_National_Forest%2C_morning_fog_shrouds_trees_%2837171636495%29.jpg/500px-Okanogan-Wenatchee_National_Forest%2C_morning_fog_shrouds_trees_%2837171636495%29.jpg 1.5x" data-file-width="5360" data-file-height="3568"></a><figcaption>Fog in the mountains</figcaption></figure>
<p>The basic intuition behind gradient descent can be illustrated by a hypothetical scenario. People are stuck in the mountains and are trying to get down (i.e., trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so they must use local information to find the minimum. They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i.e., downhill). If they were trying to find the top of the mountain (i.e., the maximum), then they would proceed in the direction of steepest ascent (i.e., uphill). Using this method, they would eventually find their way down the mountain or possibly get stuck in some hole (i.e., local minimum or <a href="/wiki/Saddle_point" title="Saddle point">saddle point</a>), like a mountain lake. However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the persons happen to have at the moment. It takes quite some time to measure the steepness of the hill with the instrument, thus they should minimize their use of the instrument if they wanted to get down the mountain before sunset. The difficulty then is choosing the frequency at which they should measure the steepness of the hill so not to go off track.
</p><p>In this analogy, the persons represent the algorithm, and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents the <a href="/wiki/Slope" title="Slope">slope</a> of the function at that point. The instrument used to measure steepness is <a href="/wiki/Differentiation_(mathematics)" class="mw-redirect" title="Differentiation (mathematics)">differentiation</a>. The direction they choose to travel in aligns with the <a href="/wiki/Gradient" title="Gradient">gradient</a> of the function at that point. The amount of time they travel before taking another measurement is the step size.
</p>
<div class="mw-heading mw-heading3"><h3 id="Choosing_the_step_size_and_descent_direction">Choosing the step size and descent direction</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=3" title="Edit section: Choosing the step size and descent direction"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Since using a step size <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span> that is too small would slow convergence, and a <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span> too large would lead to overshoot and divergence, finding a good setting of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span> is an important practical problem. <a href="/wiki/Philip_Wolfe_(mathematician)" title="Philip Wolfe (mathematician)">Philip Wolfe</a> also advocated using "clever choices of the [descent] direction" in practice.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span class="cite-bracket">[</span>10<span class="cite-bracket">]</span></a></sup> While using a direction that deviates from the steepest descent direction may seem counter-intuitive, the idea is that the smaller slope may be compensated for by being sustained over a much longer distance.
</p><p>To reason about this mathematically, consider a direction <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> and step size <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> and consider the more general update:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {a} _{n+1}=\mathbf {a} _{n}-\eta _{n}\,\mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mspace width="thinmathspace"></mspace>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {a} _{n+1}=\mathbf {a} _{n}-\eta _{n}\,\mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5d42d84397d5c8984490dedce1493fc1b7a131af" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:18.54ex; height:2.509ex;" alt="{\displaystyle \mathbf {a} _{n+1}=\mathbf {a} _{n}-\eta _{n}\,\mathbf {p} _{n}}"></span>.</dd></dl>
<p>Finding good settings of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> requires some thought. First of all, we would like the update direction to point downhill. Mathematically, letting <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \theta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>θ<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/79cc00920259451fc1a684ba7350b6f93ce4f08a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.309ex; height:2.509ex;" alt="{\displaystyle \theta _{n}}"></span> denote the angle between <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle -\nabla f(\mathbf {a_{n}} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>−<!-- − --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">a</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">n</mi>
            </mrow>
          </msub>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle -\nabla f(\mathbf {a_{n}} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be3a256ca2cd4b7d371d2b5dc6d4bd6a72ceb76" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:9.414ex; height:2.843ex;" alt="{\displaystyle -\nabla f(\mathbf {a_{n}} )}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span>, this requires that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \cos \theta _{n}>0.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>cos</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <msub>
          <mi>θ<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>&gt;</mo>
        <mn>0.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \cos \theta _{n}&gt;0.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54c11842aa128a748a96c2103dcf0a266806b6ba" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:10.715ex; height:2.509ex;" alt="{\displaystyle \cos \theta _{n}>0.}"></span> To say more, we need more information about the objective function that we are optimising. Under the fairly weak assumption that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is continuously differentiable, we may prove that:<sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span class="cite-bracket">[</span>11<span class="cite-bracket">]</span></a></sup>
</p>
<style data-mw-deduplicate="TemplateStyles:r1266403038">.mw-parser-output table.numblk{border-collapse:collapse;border:none;margin-top:0;margin-right:0;margin-bottom:0}.mw-parser-output table.numblk>tbody>tr>td{vertical-align:middle;padding:0}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2){width:99%}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table{border-collapse:collapse;margin:0;border:none;width:100%}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:first-child,.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:last-child{padding:0 0.4ex}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td:nth-child(2){width:100%;padding:0}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{padding:0}.mw-parser-output table.numblk>tbody>tr>td:last-child{font-weight:bold}.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child{font-weight:unset}.mw-parser-output table.numblk>tbody>tr>td:last-child::before{content:"("}.mw-parser-output table.numblk>tbody>tr>td:last-child::after{content:")"}.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child::before,.mw-parser-output table.numblk.numblk-raw-n>tbody>tr>td:last-child::after{content:none}.mw-parser-output table.numblk>tbody>tr>td{border:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td{border:thin solid}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td{border:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td:nth-child(2)>table>tbody>tr:first-child>td{border:thin solid}.mw-parser-output table.numblk>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{border-left:none;border-right:none;border-bottom:none}.mw-parser-output table.numblk.numblk-border>tbody>tr>td:nth-child(2)>table>tbody>tr:last-child>td{border-left:thin solid;border-right:thin solid;border-bottom:thin solid}.mw-parser-output table.numblk:target{color:var(--color-base,#202122);background-color:#cfe8fd}@media screen{html.skin-theme-clientpref-night .mw-parser-output table.numblk:target{color:var(--color-base,#eaecf0);background-color:#301702}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output table.numblk:target{color:var(--color-base,#eaecf0);background-color:#301702}}</style><table role="presentation" class="numblk" style="margin-left: 1.6em;"><tbody><tr><td class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {a} _{n+1})\leq f(\mathbf {a} _{n})-\eta _{n}\|\nabla f(\mathbf {a} _{n})\|_{2}\|\mathbf {p} _{n}\|_{2}\left(\cos \theta _{n}-\max _{t\in [0,1]}{\frac {\|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}}{\|\nabla f(\mathbf {a} _{n})\|_{2}}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>≤<!-- ≤ --></mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <msub>
          <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <msub>
          <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>cos</mi>
            <mo>⁡<!-- ⁡ --></mo>
            <msub>
              <mi>θ<!-- θ --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
            </msub>
            <mo>−<!-- − --></mo>
            <munder>
              <mo movablelimits="true" form="prefix">max</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>t</mi>
                <mo>∈<!-- ∈ --></mo>
                <mo stretchy="false">[</mo>
                <mn>0</mn>
                <mo>,</mo>
                <mn>1</mn>
                <mo stretchy="false">]</mo>
              </mrow>
            </munder>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                  <mi mathvariant="normal">∇<!-- ∇ --></mi>
                  <mi>f</mi>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">a</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </msub>
                  <mo>−<!-- − --></mo>
                  <mi>t</mi>
                  <msub>
                    <mi>η<!-- η --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </msub>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">p</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mo>−<!-- − --></mo>
                  <mi mathvariant="normal">∇<!-- ∇ --></mi>
                  <mi>f</mi>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">a</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msub>
                    <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
                <mrow>
                  <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                  <mi mathvariant="normal">∇<!-- ∇ --></mi>
                  <mi>f</mi>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">a</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msub>
                    <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {a} _{n+1})\leq f(\mathbf {a} _{n})-\eta _{n}\|\nabla f(\mathbf {a} _{n})\|_{2}\|\mathbf {p} _{n}\|_{2}\left(\cos \theta _{n}-\max _{t\in [0,1]}{\frac {\|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}}{\|\nabla f(\mathbf {a} _{n})\|_{2}}}\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f27f8bd5daff41e542b4d46a08dc1f318ccca865" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:87.075ex; height:6.509ex;" alt="{\displaystyle f(\mathbf {a} _{n+1})\leq f(\mathbf {a} _{n})-\eta _{n}\|\nabla f(\mathbf {a} _{n})\|_{2}\|\mathbf {p} _{n}\|_{2}\left(\cos \theta _{n}-\max _{t\in [0,1]}{\frac {\|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}}{\|\nabla f(\mathbf {a} _{n})\|_{2}}}\right)}"></span></td> <td></td> <td class="nowrap"><span id="math_1" class="reference nourlexpansion" style="font-weight:bold;">1</span></td></tr></tbody></table>
<p>This inequality implies that the amount by which we can be sure the function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is decreased depends on a trade off between the two terms in square brackets. The first term in square brackets measures the angle between the descent direction and the negative gradient. The second term measures how quickly the gradient changes along the descent direction.
</p><p>In principle inequality (<b><a href="#math_1">1</a></b>) could be optimized over <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> to choose an optimal step size and direction. The problem is that evaluating the second term in square brackets requires evaluating <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <mi>t</mi>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb3b13d16f89aa6c94edfd0f30d70fa1859cc07d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:16.299ex; height:2.843ex;" alt="{\displaystyle \nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})}"></span>, and extra gradient evaluations are generally expensive and undesirable. Some ways around this problem are:
</p>
<ul><li>Forgo the benefits of a clever descent direction by setting <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}=\nabla f(\mathbf {a_{n}} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">a</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">n</mi>
            </mrow>
          </msub>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}=\nabla f(\mathbf {a_{n}} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/63c5f6d185d084209f228c79cb5df6a7b465d80b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:13.408ex; height:2.843ex;" alt="{\displaystyle \mathbf {p} _{n}=\nabla f(\mathbf {a_{n}} )}"></span>, and use <a href="/wiki/Line_search" title="Line search">line search</a> to find a suitable step-size <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \gamma _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>γ<!-- γ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \gamma _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/84a6d83f89515bf8d1a053639dc484e620f42036" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.423ex; height:2.176ex;" alt="{\displaystyle \gamma _{n}}"></span>, such as one that satisfies the <a href="/wiki/Wolfe_conditions" title="Wolfe conditions">Wolfe conditions</a>. A more economic way of choosing learning rates is <a href="/wiki/Backtracking_line_search" title="Backtracking line search">backtracking line search</a>, a method that has both good theoretical guarantees and experimental results. Note that one does not need to choose  <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> to be the gradient; any direction that has positive inner product with the gradient will result in a reduction of the function value (for a sufficiently small value of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span>).</li>
<li>Assuming that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is twice-differentiable, use its Hessian <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla ^{2}f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi mathvariant="normal">∇<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla ^{2}f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f41018628f3a828a558d68269557e7cd3ec7342e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:4.269ex; height:3.009ex;" alt="{\displaystyle \nabla ^{2}f}"></span> to estimate <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}\approx \|t\eta _{n}\nabla ^{2}f(\mathbf {a} _{n})\mathbf {p} _{n}\|.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <mi>t</mi>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <msub>
          <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>≈<!-- ≈ --></mo>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mi>t</mi>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <msup>
          <mi mathvariant="normal">∇<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}\approx \|t\eta _{n}\nabla ^{2}f(\mathbf {a} _{n})\mathbf {p} _{n}\|.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba6dea727dcaa1bcc44fa0b6b71f11092a6f58f6" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:50.644ex; height:3.176ex;" alt="{\displaystyle \|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}\approx \|t\eta _{n}\nabla ^{2}f(\mathbf {a} _{n})\mathbf {p} _{n}\|.}"></span>Then choose <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> by optimising inequality (<b><a href="#math_1">1</a></b>).</li>
<li>Assuming that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7b4d6de89b52c5a5e6e1583cb63eaee263e307b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:3.214ex; height:2.509ex;" alt="{\displaystyle \nabla f}"></span> is <a href="/wiki/Lipschitz_continuity" title="Lipschitz continuity">Lipschitz</a>, use its Lipschitz constant <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle L}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>L</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/103168b86f781fe6e9a4a87b8ea1cebe0ad4ede8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.583ex; height:2.176ex;" alt="{\displaystyle L}"></span> to bound <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}\leq Lt\eta _{n}\|\mathbf {p} _{n}\|.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <mi>t</mi>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">a</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <msub>
          <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>≤<!-- ≤ --></mo>
        <mi>L</mi>
        <mi>t</mi>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}\leq Lt\eta _{n}\|\mathbf {p} _{n}\|.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ebbf2f830739e759537177a1fcff40a31de481c" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:43.631ex; height:2.843ex;" alt="{\displaystyle \|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}\leq Lt\eta _{n}\|\mathbf {p} _{n}\|.}"></span> Then choose <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> by optimising inequality (<b><a href="#math_1">1</a></b>).</li>
<li>Build a custom model of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \max _{t\in [0,1]}{\frac {\|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}}{\|\nabla f(\mathbf {a} _{n})\|_{2}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo movablelimits="true" form="prefix">max</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>∈<!-- ∈ --></mo>
            <mo stretchy="false">[</mo>
            <mn>0</mn>
            <mo>,</mo>
            <mn>1</mn>
            <mo stretchy="false">]</mo>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
              <mi mathvariant="normal">∇<!-- ∇ --></mi>
              <mi>f</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">a</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo>−<!-- − --></mo>
              <mi>t</mi>
              <msub>
                <mi>η<!-- η --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">p</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
              <mo>−<!-- − --></mo>
              <mi mathvariant="normal">∇<!-- ∇ --></mi>
              <mi>f</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">a</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
              <msub>
                <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>2</mn>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
              <mi mathvariant="normal">∇<!-- ∇ --></mi>
              <mi>f</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">a</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
              <msub>
                <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mn>2</mn>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \max _{t\in [0,1]}{\frac {\|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}}{\|\nabla f(\mathbf {a} _{n})\|_{2}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/827944aec1ba3c40ce5b61e29ebbadfc5daf4040" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -2.671ex; width:35.99ex; height:6.509ex;" alt="{\displaystyle \max _{t\in [0,1]}{\frac {\|\nabla f(\mathbf {a} _{n}-t\eta _{n}\mathbf {p} _{n})-\nabla f(\mathbf {a} _{n})\|_{2}}{\|\nabla f(\mathbf {a} _{n})\|_{2}}}}"></span> for <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span>. Then choose <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {p} _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">p</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {p} _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a0378aee34f83e9a1b77b4d73c8e20d09e33e2d" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.704ex; height:2.176ex;" alt="{\displaystyle \mathbf {p} _{n}}"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd926d56b81de76d958cf7efacd5df963f01297f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.374ex; height:2.176ex;" alt="{\displaystyle \eta _{n}}"></span> by optimising inequality (<b><a href="#math_1">1</a></b>).</li>
<li>Under stronger assumptions on the function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> such as <a href="/wiki/Convex_function" title="Convex function">convexity</a>, more <a href="#Fast_gradient_methods">advanced techniques</a> may be possible.</li></ul>
<p>Usually by following one of the recipes above, <a href="/wiki/Convergent_series" title="Convergent series">convergence</a> to a local minimum can be guaranteed. When the function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is <a href="/wiki/Convex_function" title="Convex function">convex</a>, all local minima are also global minima, so in this case gradient descent can converge to the global solution.
</p>
<div class="mw-heading mw-heading2"><h2 id="Solution_of_a_linear_system">Solution of a linear system</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=4" title="Edit section: Solution of a linear system"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<figure typeof="mw:File/Thumb"><a href="/wiki/File:Steepest_descent.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Steepest_descent.png/500px-Steepest_descent.png" decoding="async" width="380" height="304" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Steepest_descent.png/960px-Steepest_descent.png 1.5x" data-file-width="1500" data-file-height="1200"></a><figcaption>The steepest descent algorithm applied to the <a href="/wiki/Wiener_filter" title="Wiener filter">Wiener filter</a><sup id="cite_ref-12" class="reference"><a href="#cite_note-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup></figcaption></figure>
<p>Gradient descent can be used to solve a <a href="/wiki/System_of_linear_equations" title="System of linear equations">system of linear equations</a>
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} \mathbf {x} -\mathbf {b} =0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>−<!-- − --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} \mathbf {x} -\mathbf {b} =0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/435e223a0da553ca83961b0af747b0a3567bfe26" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.505ex; width:12.017ex; height:2.343ex;" alt="{\displaystyle \mathbf {A} \mathbf {x} -\mathbf {b} =0}"></span></dd></dl>
<p>reformulated as a quadratic minimization problem.
If the system matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span> is real <a href="/wiki/Symmetric_matrix" title="Symmetric matrix">symmetric</a> and <a href="/wiki/Positive-definite_matrix" class="mw-redirect" title="Positive-definite matrix">positive-definite</a>, an objective function is defined as the quadratic function, with minimization of
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {x} )=\mathbf {x} ^{\top }\mathbf {A} \mathbf {x} -2\mathbf {x} ^{\top }\mathbf {b} ,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
          </mrow>
        </msup>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>−<!-- − --></mo>
        <mn>2</mn>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
          </mrow>
        </msup>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {x} )=\mathbf {x} ^{\top }\mathbf {A} \mathbf {x} -2\mathbf {x} ^{\top }\mathbf {b} ,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba4556b22259b1326230d6e75cb40fda87073029" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:23.006ex; height:3.176ex;" alt="{\displaystyle f(\mathbf {x} )=\mathbf {x} ^{\top }\mathbf {A} \mathbf {x} -2\mathbf {x} ^{\top }\mathbf {b} ,}"></span></dd></dl>
<p>so that
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla f(\mathbf {x} )=2(\mathbf {A} \mathbf {x} -\mathbf {b} ).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>2</mn>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>−<!-- − --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla f(\mathbf {x} )=2(\mathbf {A} \mathbf {x} -\mathbf {b} ).}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/769dc711ac488ef07e64a0ddc3fe4f0c35617ca3" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:20.908ex; height:2.843ex;" alt="{\displaystyle \nabla f(\mathbf {x} )=2(\mathbf {A} \mathbf {x} -\mathbf {b} ).}"></span></dd></dl>
<p>For a general real matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span>, <a href="/wiki/Linear_least_squares" title="Linear least squares">linear least squares</a> define
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {x} )=\left\|\mathbf {A} \mathbf {x} -\mathbf {b} \right\|^{2}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msup>
          <mrow>
            <mo symmetric="true">‖</mo>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">A</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">x</mi>
              </mrow>
              <mo>−<!-- − --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">b</mi>
              </mrow>
            </mrow>
            <mo symmetric="true">‖</mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {x} )=\left\|\mathbf {A} \mathbf {x} -\mathbf {b} \right\|^{2}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8e1c13e49915329e1a406839842e788ad4f21a7f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:19.379ex; height:3.343ex;" alt="{\displaystyle f(\mathbf {x} )=\left\|\mathbf {A} \mathbf {x} -\mathbf {b} \right\|^{2}.}"></span></dd></dl>
<p>In traditional linear least squares for real <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span> and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {b} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {b} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13ebf4628a1adf07133a6009e4a78bdd990c6eb9" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="{\displaystyle \mathbf {b} }"></span> the <a href="/wiki/Euclidean_norm" class="mw-redirect" title="Euclidean norm">Euclidean norm</a> is used, in which case
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla f(\mathbf {x} )=2\mathbf {A} ^{\top }(\mathbf {A} \mathbf {x} -\mathbf {b} ).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>2</mn>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">A</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>−<!-- − --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla f(\mathbf {x} )=2\mathbf {A} ^{\top }(\mathbf {A} \mathbf {x} -\mathbf {b} ).}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e65e5b1a7819a9b8449f15b7f031f83a4ac9f190" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:24.438ex; height:3.176ex;" alt="{\displaystyle \nabla f(\mathbf {x} )=2\mathbf {A} ^{\top }(\mathbf {A} \mathbf {x} -\mathbf {b} ).}"></span></dd></dl>
<p>The <a href="/wiki/Line_search" title="Line search">line search</a> minimization, finding the locally optimal step size <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span> on every iteration, can be performed analytically for quadratic functions, and explicit formulas for the locally optimal <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;" alt="{\displaystyle \eta }"></span> are known.<sup id="cite_ref-BP_6-1" class="reference"><a href="#cite_note-BP-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-saad1996iterative_13-0" class="reference"><a href="#cite_note-saad1996iterative-13"><span class="cite-bracket">[</span>13<span class="cite-bracket">]</span></a></sup>
</p><p>For example, for real <a href="/wiki/Symmetric_matrix" title="Symmetric matrix">symmetric</a> and <a href="/wiki/Positive-definite_matrix" class="mw-redirect" title="Positive-definite matrix">positive-definite</a> matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span>, a simple algorithm can be as follows,<sup id="cite_ref-BP_6-2" class="reference"><a href="#cite_note-BP-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup>
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\begin{aligned}&amp;{\text{repeat in the loop:}}\\&amp;\qquad \mathbf {r} :=\mathbf {b} -\mathbf {Ax} \\&amp;\qquad \eta :={\mathbf {r} ^{\top }\mathbf {r} }/{\mathbf {r} ^{\top }\mathbf {Ar} }\\&amp;\qquad \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} \\&amp;\qquad {\hbox{if }}\mathbf {r} ^{\top }\mathbf {r} {\text{ is sufficiently small, then exit loop}}\\&amp;{\text{end repeat loop}}\\&amp;{\text{return }}\mathbf {x} {\text{ as the result}}\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>repeat in the loop:</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mo>−<!-- − --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">A</mi>
                  <mi mathvariant="bold">x</mi>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mi>η<!-- η --></mi>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <msup>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">r</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                    </mrow>
                  </msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold">r</mi>
                  </mrow>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <msup>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">r</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                    </mrow>
                  </msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold">A</mi>
                    <mi mathvariant="bold">r</mi>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo>+</mo>
                <mi>η<!-- η --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mrow class="MJX-TeXAtom-ORD">
                  <mstyle displaystyle="false" scriptlevel="0">
                    <mtext>if&nbsp;</mtext>
                  </mstyle>
                </mrow>
                <msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold">r</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                  </mrow>
                </msup>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>&nbsp;is sufficiently small, then exit loop</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>end repeat loop</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>return&nbsp;</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>&nbsp;as the result</mtext>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}&amp;{\text{repeat in the loop:}}\\&amp;\qquad \mathbf {r} :=\mathbf {b} -\mathbf {Ax} \\&amp;\qquad \eta :={\mathbf {r} ^{\top }\mathbf {r} }/{\mathbf {r} ^{\top }\mathbf {Ar} }\\&amp;\qquad \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} \\&amp;\qquad {\hbox{if }}\mathbf {r} ^{\top }\mathbf {r} {\text{ is sufficiently small, then exit loop}}\\&amp;{\text{end repeat loop}}\\&amp;{\text{return }}\mathbf {x} {\text{ as the result}}\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/45b2d98ecf8d9c7a974384ca0e5364e45c23277a" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -10.171ex; width:45.713ex; height:21.509ex;" alt="{\displaystyle {\begin{aligned}&amp;{\text{repeat in the loop:}}\\&amp;\qquad \mathbf {r} :=\mathbf {b} -\mathbf {Ax} \\&amp;\qquad \eta :={\mathbf {r} ^{\top }\mathbf {r} }/{\mathbf {r} ^{\top }\mathbf {Ar} }\\&amp;\qquad \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} \\&amp;\qquad {\hbox{if }}\mathbf {r} ^{\top }\mathbf {r} {\text{ is sufficiently small, then exit loop}}\\&amp;{\text{end repeat loop}}\\&amp;{\text{return }}\mathbf {x} {\text{ as the result}}\end{aligned}}}"></span></dd></dl>
<p>To avoid multiplying by <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span> twice per iteration,
we note that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>:=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>+</mo>
        <mi>η<!-- η --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">r</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eec57220e5a6166f62d330bf03b967187dd9601e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.679ex; height:2.509ex;" alt="{\displaystyle \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} }"></span> implies <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {r} :=\mathbf {r} -\eta \mathbf {Ar} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">r</mi>
        </mrow>
        <mo>:=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">r</mi>
        </mrow>
        <mo>−<!-- − --></mo>
        <mi>η<!-- η --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
          <mi mathvariant="bold">r</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {r} :=\mathbf {r} -\eta \mathbf {Ar} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/596f784bd1c487b5c58e8fc7d49da3f1803cc2b7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:13.081ex; height:2.676ex;" alt="{\displaystyle \mathbf {r} :=\mathbf {r} -\eta \mathbf {Ar} }"></span>, which gives the traditional algorithm,<sup id="cite_ref-:0_14-0" class="reference"><a href="#cite_note-:0-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup>
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\begin{aligned}&amp;\mathbf {r} :=\mathbf {b} -\mathbf {Ax} \\&amp;{\text{repeat in the loop:}}\\&amp;\qquad \eta :={\mathbf {r} ^{\top }\mathbf {r} }/{\mathbf {r} ^{\top }\mathbf {Ar} }\\&amp;\qquad \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} \\&amp;\qquad {\hbox{if }}\mathbf {r} ^{\top }\mathbf {r} {\text{ is sufficiently small, then exit loop}}\\&amp;\qquad \mathbf {r} :=\mathbf {r} -\eta \mathbf {Ar} \\&amp;{\text{end repeat loop}}\\&amp;{\text{return }}\mathbf {x} {\text{ as the result}}\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mo>−<!-- − --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">A</mi>
                  <mi mathvariant="bold">x</mi>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>repeat in the loop:</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mi>η<!-- η --></mi>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <msup>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">r</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                    </mrow>
                  </msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold">r</mi>
                  </mrow>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo>/</mo>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <msup>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="bold">r</mi>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                    </mrow>
                  </msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold">A</mi>
                    <mi mathvariant="bold">r</mi>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo>+</mo>
                <mi>η<!-- η --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mrow class="MJX-TeXAtom-ORD">
                  <mstyle displaystyle="false" scriptlevel="0">
                    <mtext>if&nbsp;</mtext>
                  </mstyle>
                </mrow>
                <msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold">r</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                  </mrow>
                </msup>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>&nbsp;is sufficiently small, then exit loop</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mspace width="2em"></mspace>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
                <mo>:=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">r</mi>
                </mrow>
                <mo>−<!-- − --></mo>
                <mi>η<!-- η --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">A</mi>
                  <mi mathvariant="bold">r</mi>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>end repeat loop</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>return&nbsp;</mtext>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>&nbsp;as the result</mtext>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}&amp;\mathbf {r} :=\mathbf {b} -\mathbf {Ax} \\&amp;{\text{repeat in the loop:}}\\&amp;\qquad \eta :={\mathbf {r} ^{\top }\mathbf {r} }/{\mathbf {r} ^{\top }\mathbf {Ar} }\\&amp;\qquad \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} \\&amp;\qquad {\hbox{if }}\mathbf {r} ^{\top }\mathbf {r} {\text{ is sufficiently small, then exit loop}}\\&amp;\qquad \mathbf {r} :=\mathbf {r} -\eta \mathbf {Ar} \\&amp;{\text{end repeat loop}}\\&amp;{\text{return }}\mathbf {x} {\text{ as the result}}\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fff635ae5452ba52667f206b2173ecca3660291" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -11.671ex; width:45.713ex; height:24.509ex;" alt="{\displaystyle {\begin{aligned}&amp;\mathbf {r} :=\mathbf {b} -\mathbf {Ax} \\&amp;{\text{repeat in the loop:}}\\&amp;\qquad \eta :={\mathbf {r} ^{\top }\mathbf {r} }/{\mathbf {r} ^{\top }\mathbf {Ar} }\\&amp;\qquad \mathbf {x} :=\mathbf {x} +\eta \mathbf {r} \\&amp;\qquad {\hbox{if }}\mathbf {r} ^{\top }\mathbf {r} {\text{ is sufficiently small, then exit loop}}\\&amp;\qquad \mathbf {r} :=\mathbf {r} -\eta \mathbf {Ar} \\&amp;{\text{end repeat loop}}\\&amp;{\text{return }}\mathbf {x} {\text{ as the result}}\end{aligned}}}"></span></dd></dl>
<p>The method is rarely used for solving linear equations, with the <a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">conjugate gradient method</a> being one of the most popular alternatives. The number of gradient descent iterations is commonly proportional to the spectral <a href="/wiki/Condition_number" title="Condition number">condition number</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \kappa (\mathbf {A} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>κ<!-- κ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \kappa (\mathbf {A} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f4a3dab1be502165a1ac693234b92db1c5ad660f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:5.168ex; height:2.843ex;" alt="{\displaystyle \kappa (\mathbf {A} )}"></span> of the system matrix <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span> (the ratio of the maximum to minimum <a href="/wiki/Eigenvalues" class="mw-redirect" title="Eigenvalues">eigenvalues</a> of <span class="nowrap"><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} ^{\top }\mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">A</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
          </mrow>
        </msup>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} ^{\top }\mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fda2f40a3140773a86e8409b54d9ea7aa148f9e" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:5.55ex; height:2.676ex;" alt="{\displaystyle \mathbf {A} ^{\top }\mathbf {A} }"></span>)</span>, while the convergence of <a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">conjugate gradient method</a> is typically determined by a square root of the condition number, i.e., is much faster. Both methods can benefit from <a href="/wiki/Preconditioner" title="Preconditioner">preconditioning</a>, where gradient descent may require less assumptions on the preconditioner.<sup id="cite_ref-:0_14-1" class="reference"><a href="#cite_note-:0-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup>
</p>
<div class="mw-heading mw-heading3"><h3 id="Geometric_behavior_and_residual_orthogonality">Geometric behavior and residual orthogonality</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=5" title="Edit section: Geometric behavior and residual orthogonality"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>In steepest descent applied to solving <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {Ax} =\mathbf {b} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {Ax} =\mathbf {b} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e68388b7df59f536a3bef4e70def2f2bb36f48c0" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:8.014ex; height:2.176ex;" alt="{\displaystyle \mathbf {Ax} =\mathbf {b} }"></span>, where <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span> is symmetric positive-definite, the residual vectors <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {r} _{k}=\mathbf {b} -\mathbf {A} \mathbf {x} _{k}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">r</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo>−<!-- − --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {r} _{k}=\mathbf {b} -\mathbf {A} \mathbf {x} _{k}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb35c202ad0912a4bb19012c7af3529163586b97" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:14.134ex; height:2.509ex;" alt="{\displaystyle \mathbf {r} _{k}=\mathbf {b} -\mathbf {A} \mathbf {x} _{k}}"></span> are orthogonal across iterations:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \langle \mathbf {r} _{k+1},\mathbf {r} _{k}\rangle =0.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">⟨<!-- ⟨ --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">r</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">r</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">⟩<!-- ⟩ --></mo>
        <mo>=</mo>
        <mn>0.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \langle \mathbf {r} _{k+1},\mathbf {r} _{k}\rangle =0.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75f9a3bb5c53033de95a00b1c183b9de5f77345b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:14.233ex; height:2.843ex;" alt="{\displaystyle \langle \mathbf {r} _{k+1},\mathbf {r} _{k}\rangle =0.}"></span></dd></dl>
<p>Because each step is taken in the steepest direction, steepest-descent steps alternate between directions aligned with the extreme axes of the elongated level sets.  When <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \kappa (\mathbf {A} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>κ<!-- κ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \kappa (\mathbf {A} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f4a3dab1be502165a1ac693234b92db1c5ad660f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:5.168ex; height:2.843ex;" alt="{\displaystyle \kappa (\mathbf {A} )}"></span> is large, this produces a characteristic zig–zag path. The poor conditioning of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span> is the primary cause of the slow convergence, and orthogonality of successive residuals reinforces this alternation.
</p>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Steepest_descent_convergence_path_for_A_%3D_2_2,_2_3.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Steepest_descent_convergence_path_for_A_%3D_2_2%2C_2_3.png/250px-Steepest_descent_convergence_path_for_A_%3D_2_2%2C_2_3.png" decoding="async" width="250" height="237" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Steepest_descent_convergence_path_for_A_%3D_2_2%2C_2_3.png/375px-Steepest_descent_convergence_path_for_A_%3D_2_2%2C_2_3.png 1.5x, //upload.wikimedia.org/wikipedia/commons/6/6c/Steepest_descent_convergence_path_for_A_%3D_2_2%2C_2_3.png 2x" data-file-width="443" data-file-height="420"></a><figcaption>Convergence path of steepest descent method for A = [[2, 2], [2, 3]]</figcaption></figure>
<p>As shown in the image on the right, steepest descent converges slowly due to the high condition number of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {A} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">A</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {A} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0795cc96c75d81520a120482662b90f024c9a1a1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="{\displaystyle \mathbf {A} }"></span>, and the orthogonality of residuals forces each new direction to undo the overshoot from the previous step. The result is a path that zigzags toward the solution. This inefficiency is one reason conjugate gradient or preconditioning methods are preferred.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15"><span class="cite-bracket">[</span>15<span class="cite-bracket">]</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Solution_of_a_non-linear_system">Solution of a non-linear system</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=6" title="Edit section: Solution of a non-linear system"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Gradient descent can also be used to solve a system of <a href="/wiki/Nonlinear_equation" class="mw-redirect" title="Nonlinear equation">nonlinear equations</a>. Below is an example that shows how to use the gradient descent to solve for three unknown variables, <i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, and <i>x</i><sub>3</sub>. This example shows one iteration of the gradient descent.
</p><p>Consider the nonlinear system of equations
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\begin{cases}3x_{1}-\cos(x_{2}x_{3})-{\tfrac {3}{2}}=0\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1=0\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}{3}}=0\end{cases}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>{</mo>
            <mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false">
              <mtr>
                <mtd>
                  <mn>3</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <mo>−<!-- − --></mo>
                  <mi>cos</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mo>−<!-- − --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mstyle displaystyle="false" scriptlevel="0">
                      <mfrac>
                        <mn>3</mn>
                        <mn>2</mn>
                      </mfrac>
                    </mstyle>
                  </mrow>
                  <mo>=</mo>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>4</mn>
                  <msubsup>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msubsup>
                  <mo>−<!-- − --></mo>
                  <mn>625</mn>
                  <msubsup>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msubsup>
                  <mo>+</mo>
                  <mn>2</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mo>−<!-- − --></mo>
                  <mn>1</mn>
                  <mo>=</mo>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>exp</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <mo>−<!-- − --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mo>+</mo>
                  <mn>20</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                  <mo>+</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mstyle displaystyle="false" scriptlevel="0">
                      <mfrac>
                        <mrow>
                          <mn>10</mn>
                          <mi>π<!-- π --></mi>
                          <mo>−<!-- − --></mo>
                          <mn>3</mn>
                        </mrow>
                        <mn>3</mn>
                      </mfrac>
                    </mstyle>
                  </mrow>
                  <mo>=</mo>
                  <mn>0</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo fence="true" stretchy="true" symmetric="true"></mo>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{cases}3x_{1}-\cos(x_{2}x_{3})-{\tfrac {3}{2}}=0\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1=0\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}{3}}=0\end{cases}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/540a1ea603de51e040c6fe8034ce8ccb10b3f4d7" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.39ex; margin-bottom: -0.282ex; width:34.928ex; height:10.509ex;" alt="{\displaystyle {\begin{cases}3x_{1}-\cos(x_{2}x_{3})-{\tfrac {3}{2}}=0\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1=0\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}{3}}=0\end{cases}}}"></span></dd></dl>
<p>Let us introduce the associated function
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle G(\mathbf {x} )={\begin{bmatrix}3x_{1}-\cos(x_{2}x_{3})-{\tfrac {3}{2}}\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}{3}}\\\end{bmatrix}},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>G</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mn>3</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <mo>−<!-- − --></mo>
                  <mi>cos</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mo>−<!-- − --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mstyle displaystyle="false" scriptlevel="0">
                      <mfrac>
                        <mn>3</mn>
                        <mn>2</mn>
                      </mfrac>
                    </mstyle>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>4</mn>
                  <msubsup>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msubsup>
                  <mo>−<!-- − --></mo>
                  <mn>625</mn>
                  <msubsup>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msubsup>
                  <mo>+</mo>
                  <mn>2</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mo>−<!-- − --></mo>
                  <mn>1</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>exp</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <mo>−<!-- − --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <mo>+</mo>
                  <mn>20</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                  <mo>+</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mstyle displaystyle="false" scriptlevel="0">
                      <mfrac>
                        <mrow>
                          <mn>10</mn>
                          <mi>π<!-- π --></mi>
                          <mo>−<!-- − --></mo>
                          <mn>3</mn>
                        </mrow>
                        <mn>3</mn>
                      </mfrac>
                    </mstyle>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle G(\mathbf {x} )={\begin{bmatrix}3x_{1}-\cos(x_{2}x_{3})-{\tfrac {3}{2}}\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}{3}}\\\end{bmatrix}},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b649372ae6174f11549dd035f107eac303b5377" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.854ex; margin-bottom: -0.317ex; width:40.494ex; height:11.509ex;" alt="{\displaystyle G(\mathbf {x} )={\begin{bmatrix}3x_{1}-\cos(x_{2}x_{3})-{\tfrac {3}{2}}\\4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\\\exp(-x_{1}x_{2})+20x_{3}+{\tfrac {10\pi -3}{3}}\\\end{bmatrix}},}"></span></dd></dl>
<p>where
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} ={\begin{bmatrix}x_{1}\\x_{2}\\x_{3}\\\end{bmatrix}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} ={\begin{bmatrix}x_{1}\\x_{2}\\x_{3}\\\end{bmatrix}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/669781ad6ed5e35cc494e28efac793c74311195f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.005ex; width:11.392ex; height:9.176ex;" alt="{\displaystyle \mathbf {x} ={\begin{bmatrix}x_{1}\\x_{2}\\x_{3}\\\end{bmatrix}}.}"></span></dd></dl>
<p>One might now define the objective function
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\begin{aligned}f(\mathbf {x} )&amp;={\frac {1}{2}}G^{\top }(\mathbf {x} )G(\mathbf {x} )\\&amp;={\frac {1}{2}}\left[\left(3x_{1}-\cos(x_{2}x_{3})-{\frac {3}{2}}\right)^{2}+\left(4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\right)^{2}+\right.\\&amp;{}\qquad \left.\left(\exp(-x_{1}x_{2})+20x_{3}+{\frac {10\pi -3}{3}}\right)^{2}\right],\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mn>1</mn>
                    <mn>2</mn>
                  </mfrac>
                </mrow>
                <msup>
                  <mi>G</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
                  </mrow>
                </msup>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo stretchy="false">)</mo>
                <mi>G</mi>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">x</mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mi></mi>
                <mo>=</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mn>1</mn>
                    <mn>2</mn>
                  </mfrac>
                </mrow>
                <mrow>
                  <mo>[</mo>
                  <mrow>
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mn>3</mn>
                          <msub>
                            <mi>x</mi>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>1</mn>
                            </mrow>
                          </msub>
                          <mo>−<!-- − --></mo>
                          <mi>cos</mi>
                          <mo>⁡<!-- ⁡ --></mo>
                          <mo stretchy="false">(</mo>
                          <msub>
                            <mi>x</mi>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>2</mn>
                            </mrow>
                          </msub>
                          <msub>
                            <mi>x</mi>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>3</mn>
                            </mrow>
                          </msub>
                          <mo stretchy="false">)</mo>
                          <mo>−<!-- − --></mo>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mfrac>
                              <mn>3</mn>
                              <mn>2</mn>
                            </mfrac>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mn>2</mn>
                      </mrow>
                    </msup>
                    <mo>+</mo>
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mn>4</mn>
                          <msubsup>
                            <mi>x</mi>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>1</mn>
                            </mrow>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>2</mn>
                            </mrow>
                          </msubsup>
                          <mo>−<!-- − --></mo>
                          <mn>625</mn>
                          <msubsup>
                            <mi>x</mi>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>2</mn>
                            </mrow>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>2</mn>
                            </mrow>
                          </msubsup>
                          <mo>+</mo>
                          <mn>2</mn>
                          <msub>
                            <mi>x</mi>
                            <mrow class="MJX-TeXAtom-ORD">
                              <mn>2</mn>
                            </mrow>
                          </msub>
                          <mo>−<!-- − --></mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mn>2</mn>
                      </mrow>
                    </msup>
                    <mo>+</mo>
                  </mrow>
                  <mo fence="true" stretchy="true" symmetric="true"></mo>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd></mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">

                </mrow>
                <mspace width="2em"></mspace>
                <mrow>
                  <mo fence="true" stretchy="true" symmetric="true"></mo>
                  <msup>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>exp</mi>
                        <mo>⁡<!-- ⁡ --></mo>
                        <mo stretchy="false">(</mo>
                        <mo>−<!-- − --></mo>
                        <msub>
                          <mi>x</mi>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mn>1</mn>
                          </mrow>
                        </msub>
                        <msub>
                          <mi>x</mi>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mn>2</mn>
                          </mrow>
                        </msub>
                        <mo stretchy="false">)</mo>
                        <mo>+</mo>
                        <mn>20</mn>
                        <msub>
                          <mi>x</mi>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mn>3</mn>
                          </mrow>
                        </msub>
                        <mo>+</mo>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mfrac>
                            <mrow>
                              <mn>10</mn>
                              <mi>π<!-- π --></mi>
                              <mo>−<!-- − --></mo>
                              <mn>3</mn>
                            </mrow>
                            <mn>3</mn>
                          </mfrac>
                        </mrow>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msup>
                  <mo>]</mo>
                </mrow>
                <mo>,</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{aligned}f(\mathbf {x} )&amp;={\frac {1}{2}}G^{\top }(\mathbf {x} )G(\mathbf {x} )\\&amp;={\frac {1}{2}}\left[\left(3x_{1}-\cos(x_{2}x_{3})-{\frac {3}{2}}\right)^{2}+\left(4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\right)^{2}+\right.\\&amp;{}\qquad \left.\left(\exp(-x_{1}x_{2})+20x_{3}+{\frac {10\pi -3}{3}}\right)^{2}\right],\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/205d7f08ca51e53e3b010ca85c55de40d4c068be" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -9.671ex; width:67.959ex; height:20.509ex;" alt="{\displaystyle {\begin{aligned}f(\mathbf {x} )&amp;={\frac {1}{2}}G^{\top }(\mathbf {x} )G(\mathbf {x} )\\&amp;={\frac {1}{2}}\left[\left(3x_{1}-\cos(x_{2}x_{3})-{\frac {3}{2}}\right)^{2}+\left(4x_{1}^{2}-625x_{2}^{2}+2x_{2}-1\right)^{2}+\right.\\&amp;{}\qquad \left.\left(\exp(-x_{1}x_{2})+20x_{3}+{\frac {10\pi -3}{3}}\right)^{2}\right],\end{aligned}}}"></span></dd></dl>
<p>which we will attempt to minimize. As an initial guess, let us use
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} ^{(0)}=\mathbf {0} ={\begin{bmatrix}0\\0\\0\\\end{bmatrix}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>0</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} ^{(0)}=\mathbf {0} ={\begin{bmatrix}0\\0\\0\\\end{bmatrix}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/828f564078b2a3d2307d9a0388a0cc76f21716c4" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.005ex; width:16.94ex; height:9.176ex;" alt="{\displaystyle \mathbf {x} ^{(0)}=\mathbf {0} ={\begin{bmatrix}0\\0\\0\\\end{bmatrix}}.}"></span></dd></dl>
<p>We know that
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} ^{(1)}=\mathbf {0} -\eta _{0}\nabla f(\mathbf {0} )=\mathbf {0} -\eta _{0}J_{G}(\mathbf {0} )^{\top }G(\mathbf {0} ),}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <msub>
          <mi>J</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>G</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">⊤<!-- ⊤ --></mi>
          </mrow>
        </msup>
        <mi>G</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} ^{(1)}=\mathbf {0} -\eta _{0}\nabla f(\mathbf {0} )=\mathbf {0} -\eta _{0}J_{G}(\mathbf {0} )^{\top }G(\mathbf {0} ),}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01a8568ba794f4fd29ffc61bd210ebfed8ecabf5" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:42.166ex; height:3.343ex;" alt="{\displaystyle \mathbf {x} ^{(1)}=\mathbf {0} -\eta _{0}\nabla f(\mathbf {0} )=\mathbf {0} -\eta _{0}J_{G}(\mathbf {0} )^{\top }G(\mathbf {0} ),}"></span></dd></dl>
<p>where the <a href="/wiki/Jacobian_matrix" class="mw-redirect" title="Jacobian matrix">Jacobian matrix</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle J_{G}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>J</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>G</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle J_{G}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eb18051dfc2735d7015023e0d0615ad6cb52af77" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:2.814ex; height:2.509ex;" alt="{\displaystyle J_{G}}"></span> is given by
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle J_{G}(\mathbf {x} )={\begin{bmatrix}3&amp;\sin(x_{2}x_{3})x_{3}&amp;\sin(x_{2}x_{3})x_{2}\\8x_{1}&amp;-1250x_{2}+2&amp;0\\-x_{2}\exp {(-x_{1}x_{2})}&amp;-x_{1}\exp(-x_{1}x_{2})&amp;20\\\end{bmatrix}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>J</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>G</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mn>3</mn>
                </mtd>
                <mtd>
                  <mi>sin</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <mi>sin</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>3</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>8</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <mn>1250</mn>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mo>+</mo>
                  <mn>2</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mi>exp</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo stretchy="false">(</mo>
                    <mo>−<!-- − --></mo>
                    <msub>
                      <mi>x</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mn>1</mn>
                      </mrow>
                    </msub>
                    <msub>
                      <mi>x</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mn>2</mn>
                      </mrow>
                    </msub>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mtd>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <mi>exp</mi>
                  <mo>⁡<!-- ⁡ --></mo>
                  <mo stretchy="false">(</mo>
                  <mo>−<!-- − --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                    </mrow>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mtd>
                <mtd>
                  <mn>20</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle J_{G}(\mathbf {x} )={\begin{bmatrix}3&amp;\sin(x_{2}x_{3})x_{3}&amp;\sin(x_{2}x_{3})x_{2}\\8x_{1}&amp;-1250x_{2}+2&amp;0\\-x_{2}\exp {(-x_{1}x_{2})}&amp;-x_{1}\exp(-x_{1}x_{2})&amp;20\\\end{bmatrix}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2bac1f9f28f50a955bea2944d1ca3d5f3918c340" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.171ex; width:63.514ex; height:9.509ex;" alt="{\displaystyle J_{G}(\mathbf {x} )={\begin{bmatrix}3&amp;\sin(x_{2}x_{3})x_{3}&amp;\sin(x_{2}x_{3})x_{2}\\8x_{1}&amp;-1250x_{2}+2&amp;0\\-x_{2}\exp {(-x_{1}x_{2})}&amp;-x_{1}\exp(-x_{1}x_{2})&amp;20\\\end{bmatrix}}.}"></span></dd></dl>
<p>We calculate:
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle J_{G}(\mathbf {0} )={\begin{bmatrix}3&amp;0&amp;0\\0&amp;2&amp;0\\0&amp;0&amp;20\end{bmatrix}},\qquad G(\mathbf {0} )={\begin{bmatrix}-2.5\\-1\\10.472\end{bmatrix}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>J</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>G</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mn>3</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>2</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>20</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mspace width="2em"></mspace>
        <mi>G</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <mn>2.5</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <mn>1</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>10.472</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle J_{G}(\mathbf {0} )={\begin{bmatrix}3&amp;0&amp;0\\0&amp;2&amp;0\\0&amp;0&amp;20\end{bmatrix}},\qquad G(\mathbf {0} )={\begin{bmatrix}-2.5\\-1\\10.472\end{bmatrix}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e90a90b31c95af2cccea00cf025c041e5228c647" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.005ex; width:46.914ex; height:9.176ex;" alt="{\displaystyle J_{G}(\mathbf {0} )={\begin{bmatrix}3&amp;0&amp;0\\0&amp;2&amp;0\\0&amp;0&amp;20\end{bmatrix}},\qquad G(\mathbf {0} )={\begin{bmatrix}-2.5\\-1\\10.472\end{bmatrix}}.}"></span></dd></dl>
<p>Thus
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} ^{(1)}=\mathbf {0} -\eta _{0}{\begin{bmatrix}-7.5\\-2\\209.44\end{bmatrix}},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <mn>7.5</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <mn>2</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>209.44</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} ^{(1)}=\mathbf {0} -\eta _{0}{\begin{bmatrix}-7.5\\-2\\209.44\end{bmatrix}},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/11c69f5345d6222e6238b4ae22bc3eb1ca0c0924" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.005ex; width:24.188ex; height:9.176ex;" alt="{\displaystyle \mathbf {x} ^{(1)}=\mathbf {0} -\eta _{0}{\begin{bmatrix}-7.5\\-2\\209.44\end{bmatrix}},}"></span></dd></dl>
<p>and
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {0} )=0.5\left((-2.5)^{2}+(-1)^{2}+(10.472)^{2}\right)=58.456.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>0.5</mn>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mo stretchy="false">(</mo>
            <mo>−<!-- − --></mo>
            <mn>2.5</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mo>−<!-- − --></mo>
            <mn>1</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>10.472</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mn>58.456.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {0} )=0.5\left((-2.5)^{2}+(-1)^{2}+(10.472)^{2}\right)=58.456.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/08d7cbf06614cc328c0955fd10d2dddeeb5ebe23" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:51.697ex; height:3.343ex;" alt="{\displaystyle f(\mathbf {0} )=0.5\left((-2.5)^{2}+(-1)^{2}+(10.472)^{2}\right)=58.456.}"></span></dd></dl>
<figure class="mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:Gradient_Descent_Example_Nonlinear_Equations.gif" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Gradient_Descent_Example_Nonlinear_Equations.gif/500px-Gradient_Descent_Example_Nonlinear_Equations.gif" decoding="async" width="350" height="251" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Gradient_Descent_Example_Nonlinear_Equations.gif/525px-Gradient_Descent_Example_Nonlinear_Equations.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/b/b5/Gradient_Descent_Example_Nonlinear_Equations.gif 2x" data-file-width="602" data-file-height="431"></a><figcaption>An animation showing the first 83 iterations of gradient descent applied to this example. Surfaces are <a href="/wiki/Isosurface" title="Isosurface">isosurfaces</a> of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {x} ^{(n)})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {x} ^{(n)})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fddab40001c0d518f35ca0208a4d4abd282faa84" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:6.997ex; height:3.343ex;" alt="{\displaystyle f(\mathbf {x} ^{(n)})}"></span> at current guess <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} ^{(n)}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} ^{(n)}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f499a0c7f0c98f9393b40fd64dd97b26f345c547" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:3.909ex; height:2.843ex;" alt="{\displaystyle \mathbf {x} ^{(n)}}"></span>, and arrows show the direction of descent. Due to a small and constant step size, the convergence is slow.</figcaption></figure>
<p>Now, a suitable <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{0}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{0}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ef3886c3f161b7f02d23221bfead21a18ffb82ff" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:2.21ex; height:2.176ex;" alt="{\displaystyle \eta _{0}}"></span> must be found such that
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f\left(\mathbf {x} ^{(1)}\right)\leq f\left(\mathbf {x} ^{(0)}\right)=f(\mathbf {0} ).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mrow>
          <mo>(</mo>
          <msup>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">(</mo>
              <mn>1</mn>
              <mo stretchy="false">)</mo>
            </mrow>
          </msup>
          <mo>)</mo>
        </mrow>
        <mo>≤<!-- ≤ --></mo>
        <mi>f</mi>
        <mrow>
          <mo>(</mo>
          <msup>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">(</mo>
              <mn>0</mn>
              <mo stretchy="false">)</mo>
            </mrow>
          </msup>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f\left(\mathbf {x} ^{(1)}\right)\leq f\left(\mathbf {x} ^{(0)}\right)=f(\mathbf {0} ).}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6b504c9de91de9e0b1dd85a96d8b778dedd84734" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.838ex; width:27.64ex; height:4.843ex;" alt="{\displaystyle f\left(\mathbf {x} ^{(1)}\right)\leq f\left(\mathbf {x} ^{(0)}\right)=f(\mathbf {0} ).}"></span></dd></dl>
<p>This can be done with any of a variety of <a href="/wiki/Line_search" title="Line search">line search</a> algorithms. One might also simply guess <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta _{0}=0.001,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>η<!-- η --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>0.001</mn>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta _{0}=0.001,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/509fb1dafab8b9f14788aca57dd06940dc1066d8" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.252ex; height:2.676ex;" alt="{\displaystyle \eta _{0}=0.001,}"></span> which gives
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {x} ^{(1)}={\begin{bmatrix}0.0075\\0.002\\-0.20944\\\end{bmatrix}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mn>0.0075</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0.002</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>−<!-- − --></mo>
                  <mn>0.20944</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} ^{(1)}={\begin{bmatrix}0.0075\\0.002\\-0.20944\\\end{bmatrix}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/685640a7600f2b9f2b69fc9bbbb1ab448af8a487" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -4.005ex; width:20.772ex; height:9.176ex;" alt="{\displaystyle \mathbf {x} ^{(1)}={\begin{bmatrix}0.0075\\0.002\\-0.20944\\\end{bmatrix}}.}"></span></dd></dl>
<p>Evaluating the objective function at this value, yields
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f\left(\mathbf {x} ^{(1)}\right)=0.5\left((-2.48)^{2}+(-1.00)^{2}+(6.28)^{2}\right)=23.306.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mrow>
          <mo>(</mo>
          <msup>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">(</mo>
              <mn>1</mn>
              <mo stretchy="false">)</mo>
            </mrow>
          </msup>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mn>0.5</mn>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mo stretchy="false">(</mo>
            <mo>−<!-- − --></mo>
            <mn>2.48</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mo>−<!-- − --></mo>
            <mn>1.00</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>6.28</mn>
            <msup>
              <mo stretchy="false">)</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mn>23.306.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f\left(\mathbf {x} ^{(1)}\right)=0.5\left((-2.48)^{2}+(-1.00)^{2}+(6.28)^{2}\right)=23.306.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/79df72924f1aa4d4fa5886bbd37ee4c9eeb3770b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.838ex; width:57.268ex; height:4.843ex;" alt="{\displaystyle f\left(\mathbf {x} ^{(1)}\right)=0.5\left((-2.48)^{2}+(-1.00)^{2}+(6.28)^{2}\right)=23.306.}"></span></dd></dl>
<p>The decrease from <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(\mathbf {0} )=58.456}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mn mathvariant="bold">0</mn>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>58.456</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(\mathbf {0} )=58.456}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54b4036d6c293c46e7535e2a269d71c1f12ecb31" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:13.982ex; height:2.843ex;" alt="{\displaystyle f(\mathbf {0} )=58.456}"></span> to the next step's value of
</p>
<dl><dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f\left(\mathbf {x} ^{(1)}\right)=23.306}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mrow>
          <mo>(</mo>
          <msup>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">(</mo>
              <mn>1</mn>
              <mo stretchy="false">)</mo>
            </mrow>
          </msup>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mn>23.306</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f\left(\mathbf {x} ^{(1)}\right)=23.306}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efa2e84afa98511a87ff55b16480cd5683a98e25" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.838ex; width:17.743ex; height:4.843ex;" alt="{\displaystyle f\left(\mathbf {x} ^{(1)}\right)=23.306}"></span></dd></dl>
<p>is a sizable decrease in the objective function. Further steps would reduce its value further until an approximate solution to the system was found.
</p>
<div class="mw-heading mw-heading2"><h2 id="Comments">Comments</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=7" title="Edit section: Comments"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Gradient descent works in spaces of any number of dimensions, even in infinite-dimensional ones. In the latter case, the search space is typically a <a href="/wiki/Function_space" title="Function space">function space</a>, and one calculates the <a href="/wiki/Fr%C3%A9chet_derivative" title="Fréchet derivative">Fréchet derivative</a> of the functional to be minimized to determine the descent direction.<sup id="cite_ref-AK82_7-1" class="reference"><a href="#cite_note-AK82-7"><span class="cite-bracket">[</span>7<span class="cite-bracket">]</span></a></sup>
</p><p>That gradient descent works in any number of dimensions (finite number at least) can be seen as a consequence of the <a href="/wiki/Cauchy%E2%80%93Schwarz_inequality" title="Cauchy–Schwarz inequality">Cauchy–Schwarz inequality</a>, i.e. the magnitude of the inner (dot) product of two vectors of any dimension is maximized when they are <a href="/wiki/Colinear" class="mw-redirect" title="Colinear">colinear</a>. In the case of gradient descent, that would be when the vector of independent variable adjustments is proportional to the gradient vector of partial derivatives.
</p><p>The gradient descent can take many iterations to compute a local minimum with a required <a href="/wiki/Accuracy" class="mw-redirect" title="Accuracy">accuracy</a>, if the <a href="/wiki/Curvature" title="Curvature">curvature</a> in different directions is very different for the given function. For such functions, <a href="/wiki/Preconditioning" class="mw-redirect" title="Preconditioning">preconditioning</a>, which changes the geometry of the space to shape the function level sets like <a href="/wiki/Concentric_circles" class="mw-redirect" title="Concentric circles">concentric circles</a>, cures the slow convergence. Constructing and applying preconditioning can be computationally expensive, however.
</p><p>The gradient descent can be modified via momentums<sup id="cite_ref-16" class="reference"><a href="#cite_note-16"><span class="cite-bracket">[</span>16<span class="cite-bracket">]</span></a></sup> (<a href="/wiki/Nesterov" title="Nesterov">Nesterov</a>, Polyak,<sup id="cite_ref-17" class="reference"><a href="#cite_note-17"><span class="cite-bracket">[</span>17<span class="cite-bracket">]</span></a></sup> and Frank–Wolfe<sup id="cite_ref-18" class="reference"><a href="#cite_note-18"><span class="cite-bracket">[</span>18<span class="cite-bracket">]</span></a></sup>) and heavy-ball parameters (exponential moving averages<sup id="cite_ref-19" class="reference"><a href="#cite_note-19"><span class="cite-bracket">[</span>19<span class="cite-bracket">]</span></a></sup> and positive-negative momentum<sup id="cite_ref-20" class="reference"><a href="#cite_note-20"><span class="cite-bracket">[</span>20<span class="cite-bracket">]</span></a></sup>). The main examples of such optimizers are Adam, DiffGrad, Yogi, AdaBelief, etc.
</p><p>Methods based on <a href="/wiki/Newton%27s_method_in_optimization" title="Newton's method in optimization">Newton's method</a> and inversion of the <a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessian</a> using <a href="/wiki/Conjugate_gradient" class="mw-redirect" title="Conjugate gradient">conjugate gradient</a> techniques can be better alternatives.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21"><span class="cite-bracket">[</span>21<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-22" class="reference"><a href="#cite_note-22"><span class="cite-bracket">[</span>22<span class="cite-bracket">]</span></a></sup> Generally, such methods converge in fewer iterations, but the cost of each iteration is higher. An example is the <a href="/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" title="Broyden–Fletcher–Goldfarb–Shanno algorithm">BFGS method</a> which consists in calculating on every step a matrix by which the gradient vector is multiplied to go into a "better" direction, combined with a more sophisticated <a href="/wiki/Line_search" title="Line search">line search</a> algorithm, to find the "best" value of <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta .}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta .}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fc94fc42a3ecbad87643808e17ec9634147cf812" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:1.816ex; height:2.176ex;" alt="{\displaystyle \eta .}"></span> For extremely large problems, where the computer-memory issues dominate, a limited-memory method such as <a href="/wiki/Limited-memory_BFGS" title="Limited-memory BFGS">L-BFGS</a> should be used instead of BFGS or the steepest descent. 
</p><p>While it is sometimes possible to substitute gradient descent for a <a href="/wiki/Local_search_(optimization)" title="Local search (optimization)">local search</a> algorithm, gradient descent is not in the same family: although it is an <a href="/wiki/Iterative_method" title="Iterative method">iterative method</a> for <a href="/wiki/Global_optimization" title="Global optimization">local optimization</a>, it relies on an <a href="/wiki/Loss_function" title="Loss function">objective function’s gradient</a> rather than an explicit exploration of a <a href="/wiki/Feasible_region" title="Feasible region">solution space</a>.
</p><p>Gradient descent can be viewed as applying <a href="/wiki/Euler%27s_method" class="mw-redirect" title="Euler's method">Euler's method</a> for solving <a href="/wiki/Ordinary_differential_equations" class="mw-redirect" title="Ordinary differential equations">ordinary differential equations</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x'(t)=-\nabla f(x(t))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>x</mi>
          <mo>′</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x'(t)=-\nabla f(x(t))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f7efd24e9cf34508df8d402a5b9801290acb2afc" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:18.572ex; height:3.009ex;" alt="{\displaystyle x'(t)=-\nabla f(x(t))}"></span> to a <a href="/wiki/Gradient_flow" class="mw-redirect" title="Gradient flow">gradient flow</a>.  In turn, this equation may be derived as an optimal controller<sup id="cite_ref-23" class="reference"><a href="#cite_note-23"><span class="cite-bracket">[</span>23<span class="cite-bracket">]</span></a></sup> for the control system <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x'(t)=u(t)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>x</mi>
          <mo>′</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>u</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x'(t)=u(t)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/861cfd9214a74c07501a0b53535245f7d96715ec" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:11.74ex; height:3.009ex;" alt="{\displaystyle x'(t)=u(t)}"></span> with <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle u(t)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>u</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle u(t)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b375df3b65d282f8715835dc91ccb22f46993959" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:3.979ex; height:2.843ex;" alt="{\displaystyle u(t)}"></span> given in feedback form <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle u(t)=-\nabla f(x(t))}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>u</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle u(t)=-\nabla f(x(t))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f86ee85b8175055e73c54efb92404922a835f7dd" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:17.888ex; height:2.843ex;" alt="{\displaystyle u(t)=-\nabla f(x(t))}"></span>.
</p><p><br>
</p>
<div class="mw-heading mw-heading2"><h2 id="Modifications">Modifications</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=8" title="Edit section: Modifications"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Gradient descent can converge to a local minimum and slow down in a neighborhood of a <a href="/wiki/Saddle_point" title="Saddle point">saddle point</a>. Even for unconstrained quadratic minimization, gradient descent develops a zig–zag pattern of subsequent iterates as iterations progress, resulting in slow convergence. Multiple modifications of gradient descent have been proposed to address these deficiencies.
</p>
<div class="mw-heading mw-heading3"><h3 id="Fast_gradient_methods">Fast gradient methods</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=9" title="Edit section: Fast gradient methods"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p><a href="/wiki/Yurii_Nesterov" title="Yurii Nesterov">Yurii Nesterov</a> has proposed<sup id="cite_ref-24" class="reference"><a href="#cite_note-24"><span class="cite-bracket">[</span>24<span class="cite-bracket">]</span></a></sup> a simple modification that enables faster convergence for convex problems and has been since further generalized. For unconstrained smooth problems, the method is called the <a href="/w/index.php?title=Fast_gradient_method&amp;action=edit&amp;redlink=1" class="new" title="Fast gradient method (page does not exist)">fast gradient method</a> (FGM) or the <a href="/w/index.php?title=Accelerated_gradient_method&amp;action=edit&amp;redlink=1" class="new" title="Accelerated gradient method (page does not exist)">accelerated gradient method</a> (AGM). Specifically, if the differentiable function <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is convex and <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \nabla f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∇<!-- ∇ --></mi>
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \nabla f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7b4d6de89b52c5a5e6e1583cb63eaee263e307b" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:3.214ex; height:2.509ex;" alt="{\displaystyle \nabla f}"></span> is <a href="/wiki/Lipschitz_continuity" title="Lipschitz continuity">Lipschitz</a>, and it is not assumed that <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:1.279ex; height:2.509ex;" alt="{\displaystyle f}"></span> is <a href="/wiki/Convex_function#Strongly_convex_functions" title="Convex function">strongly convex</a>, then the error in the objective value generated at each step <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;" alt="{\displaystyle k}"></span> by the gradient descent method will be <a href="/wiki/Big_O_notation" title="Big O notation">bounded by</a> <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle {\mathcal {O}}\left({k^{-1}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>k</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>−<!-- − --></mo>
                <mn>1</mn>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle {\mathcal {O}}\left({k^{-1}}\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/88d4397c2f60be08675706a530f8029e45e25e00" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:7.911ex; height:3.176ex;" alt="{\textstyle {\mathcal {O}}\left({k^{-1}}\right)}"></span>. Using the Nesterov acceleration technique, the error decreases at <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle {\mathcal {O}}\left({k^{-2}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>k</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>−<!-- − --></mo>
                <mn>2</mn>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle {\mathcal {O}}\left({k^{-2}}\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/25c553eab8e24c9b73b4b543a86d4cf8ea56324f" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:7.911ex; height:3.176ex;" alt="{\textstyle {\mathcal {O}}\left({k^{-2}}\right)}"></span>.<sup id="cite_ref-25" class="reference"><a href="#cite_note-25"><span class="cite-bracket">[</span>25<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-26" class="reference"><a href="#cite_note-26"><span class="cite-bracket">[</span>26<span class="cite-bracket">]</span></a></sup> It is known that the rate <span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\mathcal {O}}\left({k^{-2}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mrow>
          <mo>(</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>k</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>−<!-- − --></mo>
                <mn>2</mn>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}\left({k^{-2}}\right)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3f99834ce0fe13b17fc7d4fc3fcf51ea74ced8d1" class="mwe-math-fallback-image-inline mw-invert skin-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:7.911ex; height:3.343ex;" alt="{\displaystyle {\mathcal {O}}\left({k^{-2}}\right)}"></span> for the decrease of the <a href="/wiki/Loss_function" title="Loss function">cost function</a> is optimal for first-order optimization methods. Nevertheless, there is the opportunity to improve the algorithm by reducing the constant factor. The <a href="/w/index.php?title=Optimized_gradient_method&amp;action=edit&amp;redlink=1" class="new" title="Optimized gradient method (page does not exist)">optimized gradient method</a> (OGM)<sup id="cite_ref-27" class="reference"><a href="#cite_note-27"><span class="cite-bracket">[</span>27<span class="cite-bracket">]</span></a></sup> reduces that constant by a factor of two and is an optimal first-order method for large-scale problems.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28"><span class="cite-bracket">[</span>28<span class="cite-bracket">]</span></a></sup>
</p><p>For constrained or non-smooth problems, Nesterov's FGM is called the <a href="/w/index.php?title=Fast_proximal_gradient_method&amp;action=edit&amp;redlink=1" class="new" title="Fast proximal gradient method (page does not exist)">fast proximal gradient method</a> (FPGM), an acceleration of the <a href="/wiki/Proximal_gradient_method" title="Proximal gradient method">proximal gradient method</a>.
</p>
<div class="mw-heading mw-heading3"><h3 id="Momentum_or_heavy_ball_method">Momentum or <i>heavy ball</i> method</h3><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=10" title="Edit section: Momentum or heavy ball method"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Trying to break the zig-zag pattern of gradient descent, the <i>momentum or heavy ball method</i> uses a momentum term in analogy to a heavy ball sliding on the surface of values of the function being minimized,<sup id="cite_ref-BP_6-3" class="reference"><a href="#cite_note-BP-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup> or to mass movement in <a href="/wiki/Newtonian_dynamics" title="Newtonian dynamics">Newtonian dynamics</a> through a <a href="/wiki/Viscous" class="mw-redirect" title="Viscous">viscous</a> medium in a <a href="/wiki/Conservative_force" title="Conservative force">conservative force</a> field.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29"><span class="cite-bracket">[</span>29<span class="cite-bracket">]</span></a></sup> Gradient descent with momentum remembers the solution update at each iteration, and determines the next update as a <a href="/wiki/Linear_combination" title="Linear combination">linear combination</a> of the gradient and the previous update. For unconstrained quadratic minimization, a theoretical convergence rate bound of the heavy ball method is asymptotically the same as that for the optimal <a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">conjugate gradient method</a>.<sup id="cite_ref-BP_6-4" class="reference"><a href="#cite_note-BP-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup>
</p><p>This technique is used in <a href="/wiki/Stochastic_gradient_descent#Momentum" title="Stochastic gradient descent">stochastic gradient descent</a> and as an extension to the <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> algorithms used to train <a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">artificial neural networks</a>.<sup id="cite_ref-30" class="reference"><a href="#cite_note-30"><span class="cite-bracket">[</span>30<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-31" class="reference"><a href="#cite_note-31"><span class="cite-bracket">[</span>31<span class="cite-bracket">]</span></a></sup> In the direction of updating, stochastic gradient descent adds a stochastic property. The weights can be used to calculate the derivatives.
</p>
<div class="mw-heading mw-heading2"><h2 id="Extensions">Extensions</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=11" title="Edit section: Extensions"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>Gradient descent can be extended to handle <a href="/wiki/Constraint_(mathematics)" title="Constraint (mathematics)">constraints</a> by including a <a href="/wiki/Projection_(linear_algebra)" title="Projection (linear algebra)">projection</a> onto the set of constraints. This method is only feasible when the projection is efficiently computable on a computer. Under suitable assumptions, this method converges.  This method is a specific case of the <a href="/wiki/Forward%E2%80%93backward_algorithm" title="Forward–backward algorithm">forward–backward algorithm</a> for monotone inclusions (which includes <a href="/wiki/Convex_programming" class="mw-redirect" title="Convex programming">convex programming</a> and <a href="/wiki/Variational_inequality" title="Variational inequality">variational inequalities</a>).<sup id="cite_ref-32" class="reference"><a href="#cite_note-32"><span class="cite-bracket">[</span>32<span class="cite-bracket">]</span></a></sup>
</p><p>Gradient descent is a special case of <a href="/wiki/Mirror_descent" title="Mirror descent">mirror descent</a> using the squared Euclidean distance as the given <a href="/wiki/Bregman_divergence" title="Bregman divergence">Bregman divergence</a>.<sup id="cite_ref-33" class="reference"><a href="#cite_note-33"><span class="cite-bracket">[</span>33<span class="cite-bracket">]</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="Theoretical_properties">Theoretical properties</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=12" title="Edit section: Theoretical properties"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<p>The properties of gradient descent depend on the properties of the objective function and the variant of gradient descent used (for example, if a <a href="/wiki/Line_search" title="Line search">line search</a> step is used). The assumptions made affect the convergence rate, and other properties, that can be proven for gradient descent.<sup id="cite_ref-:1_34-0" class="reference"><a href="#cite_note-:1-34"><span class="cite-bracket">[</span>34<span class="cite-bracket">]</span></a></sup> For example, if the objective is assumed to be <a href="/wiki/Strongly_convex_function" class="mw-redirect" title="Strongly convex function">strongly convex</a> and <a href="/wiki/Lipschitz_continuity" title="Lipschitz continuity">lipschitz smooth</a>, then gradient descent converges linearly with a fixed step size.<sup id="cite_ref-auto_1-1" class="reference"><a href="#cite_note-auto-1"><span class="cite-bracket">[</span>1<span class="cite-bracket">]</span></a></sup> Looser assumptions lead to either weaker convergence guarantees or require a more sophisticated step size selection.<sup id="cite_ref-:1_34-1" class="reference"><a href="#cite_note-:1-34"><span class="cite-bracket">[</span>34<span class="cite-bracket">]</span></a></sup>
</p>
<div class="mw-heading mw-heading2"><h2 id="See_also">See also</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=13" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1184024115">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style><div class="div-col" style="column-width: 20em;">
<ul><li><a href="/wiki/Backtracking_line_search" title="Backtracking line search">Backtracking line search</a></li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient method</a></li>
<li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">Stochastic gradient descent</a></li>
<li><a href="/wiki/Rprop" title="Rprop">Rprop</a></li>
<li><a href="/wiki/Delta_rule" title="Delta rule">Delta rule</a></li>
<li><a href="/wiki/Wolfe_conditions" title="Wolfe conditions">Wolfe conditions</a></li>
<li><a href="/wiki/Preconditioning" class="mw-redirect" title="Preconditioning">Preconditioning</a></li>
<li><a href="/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" title="Broyden–Fletcher–Goldfarb–Shanno algorithm">Broyden–Fletcher–Goldfarb–Shanno algorithm</a></li>
<li><a href="/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula" title="Davidon–Fletcher–Powell formula">Davidon–Fletcher–Powell formula</a></li>
<li><a href="/wiki/Nelder%E2%80%93Mead_method" title="Nelder–Mead method">Nelder–Mead method</a></li>
<li><a href="/wiki/Gauss%E2%80%93Newton_algorithm" title="Gauss–Newton algorithm">Gauss–Newton algorithm</a></li>
<li><a href="/wiki/Hill_climbing" title="Hill climbing">Hill climbing</a></li>
<li><a href="/wiki/Quantum_annealing" title="Quantum annealing">Quantum annealing</a></li>
<li><a href="/wiki/TFNP#CLS" title="TFNP">CLS</a> (continuous local search)</li>
<li><a href="/wiki/Neuroevolution" title="Neuroevolution">Neuroevolution</a></li></ul>
</div>
<div class="mw-heading mw-heading2"><h2 id="References">References</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=14" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1239543626">.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist reflist-columns references-column-width" style="column-width: 30em;">
<ol class="references">
<li id="cite_note-auto-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-auto_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-auto_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1238218222">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id="CITEREFBoydVandenberghe2004" class="citation book cs1">Boyd, Stephen; Vandenberghe, Lieven (2004-03-08). <a rel="nofollow" class="external text" href="https://dx.doi.org/10.1017/cbo9780511804441"><i>Convex Optimization</i></a>. Cambridge University Press. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1017%2Fcbo9780511804441">10.1017/cbo9780511804441</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-0-521-83378-3" title="Special:BookSources/978-0-521-83378-3"><bdi>978-0-521-83378-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Convex+Optimization&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2004-03-08&amp;rft_id=info%3Adoi%2F10.1017%2Fcbo9780511804441&amp;rft.isbn=978-0-521-83378-3&amp;rft.aulast=Boyd&amp;rft.aufirst=Stephen&amp;rft.au=Vandenberghe%2C+Lieven&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.1017%2Fcbo9780511804441&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFLemaréchal,_C.2012" class="citation book cs1 cs1-prop-vanc-accept">Lemaréchal, C. (1 January 2012). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20181229073335/https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">"Cauchy and the gradient method"</a> <span class="cs1-format">(PDF)</span>. In Grötschel, M. (ed.). <i>Optimization Stories</i>. Documenta Mathematica Series. Vol.&nbsp;6 (1st&nbsp;ed.). EMS Press. pp.&nbsp;<span class="nowrap">251–</span>254. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.4171%2Fdms%2F6%2F27">10.4171/dms/6/27</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-3-936609-58-5" title="Special:BookSources/978-3-936609-58-5"><bdi>978-3-936609-58-5</bdi></a>. Archived from <a rel="nofollow" class="external text" href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">the original</a> <span class="cs1-format">(PDF)</span> on 2018-12-29<span class="reference-accessdate">. Retrieved <span class="nowrap">2020-01-26</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Cauchy+and+the+gradient+method&amp;rft.btitle=Optimization+Stories&amp;rft.series=Documenta+Mathematica+Series&amp;rft.pages=251-254&amp;rft.edition=1st&amp;rft.pub=EMS+Press&amp;rft.date=2012-01-01&amp;rft_id=info%3Adoi%2F10.4171%2Fdms%2F6%2F27&amp;rft.isbn=978-3-936609-58-5&amp;rft.au=Lemar%C3%A9chal%2C+C.&amp;rft_id=https%3A%2F%2Fwww.math.uni-bielefeld.de%2Fdocumenta%2Fvol-ismp%2F40_lemarechal-claude.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFHadamard1908" class="citation journal cs1">Hadamard, Jacques (1908). "Mémoire sur le problème d'analyse relatif à l'équilibre des plaques élastiques encastrées". <i>Mémoires présentés par divers savants éstrangers à l'Académie des Sciences de l'Institut de France</i>. <b>33</b>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=M%C3%A9moires+pr%C3%A9sent%C3%A9s+par+divers+savants+%C3%A9strangers+%C3%A0+l%27Acad%C3%A9mie+des+Sciences+de+l%27Institut+de+France&amp;rft.atitle=M%C3%A9moire+sur+le+probl%C3%A8me+d%27analyse+relatif+%C3%A0+l%27%C3%A9quilibre+des+plaques+%C3%A9lastiques+encastr%C3%A9es&amp;rft.volume=33&amp;rft.date=1908&amp;rft.aulast=Hadamard&amp;rft.aufirst=Jacques&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFCourant1943" class="citation journal cs1">Courant, R. (1943). <a rel="nofollow" class="external text" href="https://doi.org/10.1090%2FS0002-9904-1943-07818-4">"Variational methods for the solution of problems of equilibrium and vibrations"</a>. <i>Bulletin of the American Mathematical Society</i>. <b>49</b> (1): <span class="nowrap">1–</span>23. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1090%2FS0002-9904-1943-07818-4">10.1090/S0002-9904-1943-07818-4</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+the+American+Mathematical+Society&amp;rft.atitle=Variational+methods+for+the+solution+of+problems+of+equilibrium+and+vibrations&amp;rft.volume=49&amp;rft.issue=1&amp;rft.pages=1-23&amp;rft.date=1943&amp;rft_id=info%3Adoi%2F10.1090%2FS0002-9904-1943-07818-4&amp;rft.aulast=Courant&amp;rft.aufirst=R.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1090%252FS0002-9904-1943-07818-4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFCurry1944" class="citation journal cs1">Curry, Haskell B. (1944). <a rel="nofollow" class="external text" href="https://doi.org/10.1090%2Fqam%2F10667">"The Method of Steepest Descent for Non-linear Minimization Problems"</a>. <i>Quart. Appl. Math</i>. <b>2</b> (3): <span class="nowrap">258–</span>261. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1090%2Fqam%2F10667">10.1090/qam/10667</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Quart.+Appl.+Math.&amp;rft.atitle=The+Method+of+Steepest+Descent+for+Non-linear+Minimization+Problems&amp;rft.volume=2&amp;rft.issue=3&amp;rft.pages=258-261&amp;rft.date=1944&amp;rft_id=info%3Adoi%2F10.1090%2Fqam%2F10667&amp;rft.aulast=Curry&amp;rft.aufirst=Haskell+B.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1090%252Fqam%252F10667&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-BP-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-BP_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-BP_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-BP_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-BP_6-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-BP_6-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFPolyak1987" class="citation book cs1"><a href="/w/index.php?title=Boris_T._Polyak&amp;action=edit&amp;redlink=1" class="new" title="Boris T. Polyak (page does not exist)">Polyak, Boris</a> (1987). <a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/342978480"><i>Introduction to Optimization</i></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Optimization&amp;rft.date=1987&amp;rft.aulast=Polyak&amp;rft.aufirst=Boris&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F342978480&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-AK82-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-AK82_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AK82_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFAkilovKantorovich1982" class="citation book cs1">Akilov, G. P.; <a href="/wiki/Leonid_Kantorovich" title="Leonid Kantorovich">Kantorovich, L. V.</a> (1982). <i>Functional Analysis</i> (2nd&nbsp;ed.). Pergamon Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/0-08-023036-9" title="Special:BookSources/0-08-023036-9"><bdi>0-08-023036-9</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Functional+Analysis&amp;rft.edition=2nd&amp;rft.pub=Pergamon+Press&amp;rft.date=1982&amp;rft.isbn=0-08-023036-9&amp;rft.aulast=Akilov&amp;rft.aufirst=G.+P.&amp;rft.au=Kantorovich%2C+L.+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFBarzilaiBorwein1988" class="citation journal cs1">Barzilai, Jonathan; Borwein, Jonathan M. (1988). "Two-Point Step Size Gradient Methods". <i>IMA Journal of Numerical Analysis</i>. <b>8</b> (1): <span class="nowrap">141–</span>148. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1093%2Fimanum%2F8.1.141">10.1093/imanum/8.1.141</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IMA+Journal+of+Numerical+Analysis&amp;rft.atitle=Two-Point+Step+Size+Gradient+Methods&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=141-148&amp;rft.date=1988&amp;rft_id=info%3Adoi%2F10.1093%2Fimanum%2F8.1.141&amp;rft.aulast=Barzilai&amp;rft.aufirst=Jonathan&amp;rft.au=Borwein%2C+Jonathan+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFFletcher2005" class="citation book cs1">Fletcher, R. (2005). "On the Barzilai–Borwein Method". In Qi, L.; Teo, K.; Yang, X. (eds.). <i>Optimization and Control with Applications</i>. Applied Optimization. Vol.&nbsp;96. Boston: Springer. pp.&nbsp;<span class="nowrap">235–</span>256. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/0-387-24254-6" title="Special:BookSources/0-387-24254-6"><bdi>0-387-24254-6</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=On+the+Barzilai%E2%80%93Borwein+Method&amp;rft.btitle=Optimization+and+Control+with+Applications&amp;rft.place=Boston&amp;rft.series=Applied+Optimization&amp;rft.pages=235-256&amp;rft.pub=Springer&amp;rft.date=2005&amp;rft.isbn=0-387-24254-6&amp;rft.aulast=Fletcher&amp;rft.aufirst=R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFWolfe1969" class="citation journal cs1">Wolfe, Philip (April 1969). "Convergence Conditions for Ascent Methods". <i>SIAM Review</i>. <b>11</b> (2): <span class="nowrap">226–</span>235. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137%2F1011036">10.1137/1011036</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Review&amp;rft.atitle=Convergence+Conditions+for+Ascent+Methods&amp;rft.volume=11&amp;rft.issue=2&amp;rft.pages=226-235&amp;rft.date=1969-04&amp;rft_id=info%3Adoi%2F10.1137%2F1011036&amp;rft.aulast=Wolfe&amp;rft.aufirst=Philip&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFBernsteinVahdatYueLiu2020" class="citation arxiv cs1">Bernstein, Jeremy; Vahdat, Arash; Yue, Yisong; Liu, Ming-Yu (2020-06-12). "On the distance between two neural networks and the stability of learning". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2002.03432">2002.03432</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=On+the+distance+between+two+neural+networks+and+the+stability+of+learning&amp;rft.date=2020-06-12&amp;rft_id=info%3Aarxiv%2F2002.03432&amp;rft.aulast=Bernstein&amp;rft.aufirst=Jeremy&amp;rft.au=Vahdat%2C+Arash&amp;rft.au=Yue%2C+Yisong&amp;rft.au=Liu%2C+Ming-Yu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">Haykin, Simon S. Adaptive filter theory. Pearson Education India, 2008. - p. 108-142, 217-242</span>
</li>
<li id="cite_note-saad1996iterative-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-saad1996iterative_13-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFSaad2003" class="citation book cs1">Saad, Yousef (2003). <a rel="nofollow" class="external text" href="https://archive.org/details/iterativemethods0000saad/page/195"><i>Iterative methods for sparse linear systems</i></a> (2nd&nbsp;ed.). Philadelphia, Pa.: Society for Industrial and Applied Mathematics. pp.&nbsp;<a rel="nofollow" class="external text" href="https://archive.org/details/iterativemethods0000saad/page/195">195</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-0-89871-534-7" title="Special:BookSources/978-0-89871-534-7"><bdi>978-0-89871-534-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Iterative+methods+for+sparse+linear+systems&amp;rft.place=Philadelphia%2C+Pa.&amp;rft.pages=195&amp;rft.edition=2nd&amp;rft.pub=Society+for+Industrial+and+Applied+Mathematics&amp;rft.date=2003&amp;rft.isbn=978-0-89871-534-7&amp;rft.aulast=Saad&amp;rft.aufirst=Yousef&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fiterativemethods0000saad%2Fpage%2F195&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-:0-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_14-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFBouwmeesterDoughertyKnyazev2015" class="citation journal cs1">Bouwmeester, Henricus; Dougherty, Andrew; Knyazev, Andrew V. (2015). <a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.procs.2015.05.241">"Nonsymmetric Preconditioning for Conjugate Gradient and Steepest Descent Methods"</a>. <i>Procedia Computer Science</i>. <b>51</b>: <span class="nowrap">276–</span>285. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1212.6680">1212.6680</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.procs.2015.05.241">10.1016/j.procs.2015.05.241</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Procedia+Computer+Science&amp;rft.atitle=Nonsymmetric+Preconditioning+for+Conjugate+Gradient+and+Steepest+Descent+Methods&amp;rft.volume=51&amp;rft.pages=276-285&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1212.6680&amp;rft_id=info%3Adoi%2F10.1016%2Fj.procs.2015.05.241&amp;rft.aulast=Bouwmeester&amp;rft.aufirst=Henricus&amp;rft.au=Dougherty%2C+Andrew&amp;rft.au=Knyazev%2C+Andrew+V.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.procs.2015.05.241&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFHolmes,_M.2023" class="citation book cs1">Holmes, M. (2023). <i>Introduction to Scientific Computing and Data Analysis, 2nd Ed</i>. Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-3-031-22429-4" title="Special:BookSources/978-3-031-22429-4"><bdi>978-3-031-22429-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Scientific+Computing+and+Data+Analysis%2C+2nd+Ed&amp;rft.pub=Springer&amp;rft.date=2023&amp;rft.isbn=978-3-031-22429-4&amp;rft.au=Holmes%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFAbdulkadirovLyakhovNagornov2023" class="citation journal cs1">Abdulkadirov, Ruslan; Lyakhov, Pavel; Nagornov, Nikolay (January 2023). <a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Fmath11112466">"Survey of Optimization Algorithms in Modern Neural Networks"</a>. <i>Mathematics</i>. <b>11</b> (11): 2466. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Fmath11112466">10.3390/math11112466</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/2227-7390">2227-7390</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics&amp;rft.atitle=Survey+of+Optimization+Algorithms+in+Modern+Neural+Networks&amp;rft.volume=11&amp;rft.issue=11&amp;rft.pages=2466&amp;rft.date=2023-01&amp;rft_id=info%3Adoi%2F10.3390%2Fmath11112466&amp;rft.issn=2227-7390&amp;rft.aulast=Abdulkadirov&amp;rft.aufirst=Ruslan&amp;rft.au=Lyakhov%2C+Pavel&amp;rft.au=Nagornov%2C+Nikolay&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.3390%252Fmath11112466&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFDiakonikolasJordan2021" class="citation journal cs1">Diakonikolas, Jelena; Jordan, Michael I. (January 2021). <a rel="nofollow" class="external text" href="https://epubs.siam.org/doi/10.1137/20M1322716">"Generalized Momentum-Based Methods: A Hamiltonian Perspective"</a>. <i>SIAM Journal on Optimization</i>. <b>31</b> (1): <span class="nowrap">915–</span>944. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1906.00436">1906.00436</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137%2F20M1322716">10.1137/20M1322716</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/1052-6234">1052-6234</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Optimization&amp;rft.atitle=Generalized+Momentum-Based+Methods%3A+A+Hamiltonian+Perspective&amp;rft.volume=31&amp;rft.issue=1&amp;rft.pages=915-944&amp;rft.date=2021-01&amp;rft_id=info%3Aarxiv%2F1906.00436&amp;rft.issn=1052-6234&amp;rft_id=info%3Adoi%2F10.1137%2F20M1322716&amp;rft.aulast=Diakonikolas&amp;rft.aufirst=Jelena&amp;rft.au=Jordan%2C+Michael+I.&amp;rft_id=https%3A%2F%2Fepubs.siam.org%2Fdoi%2F10.1137%2F20M1322716&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFMeyer1974" class="citation journal cs1">Meyer, Gerard G. L. (November 1974). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="http://epubs.siam.org/doi/10.1137/0312050">"Accelerated Frank–Wolfe Algorithms"</a></span>. <i>SIAM Journal on Control</i>. <b>12</b> (4): <span class="nowrap">655–</span>663. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137%2F0312050">10.1137/0312050</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0036-1402">0036-1402</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Control&amp;rft.atitle=Accelerated+Frank%E2%80%93Wolfe+Algorithms&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=655-663&amp;rft.date=1974-11&amp;rft_id=info%3Adoi%2F10.1137%2F0312050&amp;rft.issn=0036-1402&amp;rft.aulast=Meyer&amp;rft.aufirst=Gerard+G.+L.&amp;rft_id=http%3A%2F%2Fepubs.siam.org%2Fdoi%2F10.1137%2F0312050&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFKingmaBa2017" class="citation cs2">Kingma, Diederik P.; Ba, Jimmy (2017-01-29), <i>Adam: A Method for Stochastic Optimization</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1412.6980">1412.6980</a></span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Adam%3A+A+Method+for+Stochastic+Optimization&amp;rft.date=2017-01-29&amp;rft_id=info%3Aarxiv%2F1412.6980&amp;rft.aulast=Kingma&amp;rft.aufirst=Diederik+P.&amp;rft.au=Ba%2C+Jimmy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFXieYuanZhuSugiyama2021" class="citation journal cs1">Xie, Zeke; Yuan, Li; Zhu, Zhanxing; Sugiyama, Masashi (2021-07-01). <a rel="nofollow" class="external text" href="https://proceedings.mlr.press/v139/xie21h.html">"Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization"</a>. <i>Proceedings of the 38th International Conference on Machine Learning</i>. PMLR: <span class="nowrap">11448–</span>11458. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2103.17182">2103.17182</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+38th+International+Conference+on+Machine+Learning&amp;rft.atitle=Positive-Negative+Momentum%3A+Manipulating+Stochastic+Gradient+Noise+to+Improve+Generalization&amp;rft.pages=11448-11458&amp;rft.date=2021-07-01&amp;rft_id=info%3Aarxiv%2F2103.17182&amp;rft.aulast=Xie&amp;rft.aufirst=Zeke&amp;rft.au=Yuan%2C+Li&amp;rft.au=Zhu%2C+Zhanxing&amp;rft.au=Sugiyama%2C+Masashi&amp;rft_id=https%3A%2F%2Fproceedings.mlr.press%2Fv139%2Fxie21h.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFPressTeukolskyVetterlingFlannery1992" class="citation book cs1"><a href="/wiki/William_H._Press" title="William H. Press">Press, W. H.</a>; <a href="/wiki/Saul_Teukolsky" title="Saul Teukolsky">Teukolsky, S. A.</a>; Vetterling, W. T.; <a href="/wiki/Brian_P._Flannery" title="Brian P. Flannery">Flannery, B. P.</a> (1992). <span class="id-lock-registration" title="Free registration required"><a rel="nofollow" class="external text" href="https://archive.org/details/numericalrecipes00pres_0"><i>Numerical Recipes in C: The Art of Scientific Computing</i></a></span> (2nd&nbsp;ed.). New York: <a href="/wiki/Cambridge_University_Press" title="Cambridge University Press">Cambridge University Press</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/0-521-43108-5" title="Special:BookSources/0-521-43108-5"><bdi>0-521-43108-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Numerical+Recipes+in+C%3A+The+Art+of+Scientific+Computing&amp;rft.place=New+York&amp;rft.edition=2nd&amp;rft.pub=Cambridge+University+Press&amp;rft.date=1992&amp;rft.isbn=0-521-43108-5&amp;rft.aulast=Press&amp;rft.aufirst=W.+H.&amp;rft.au=Teukolsky%2C+S.+A.&amp;rft.au=Vetterling%2C+W.+T.&amp;rft.au=Flannery%2C+B.+P.&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fnumericalrecipes00pres_0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFStrutz2016" class="citation book cs1">Strutz, T. (2016). <i>Data Fitting and Uncertainty: A Practical Introduction to Weighted Least Squares and Beyond</i> (2nd&nbsp;ed.). Springer Vieweg. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-3-658-11455-8" title="Special:BookSources/978-3-658-11455-8"><bdi>978-3-658-11455-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Data+Fitting+and+Uncertainty%3A+A+Practical+Introduction+to+Weighted+Least+Squares+and+Beyond&amp;rft.edition=2nd&amp;rft.pub=Springer+Vieweg&amp;rft.date=2016&amp;rft.isbn=978-3-658-11455-8&amp;rft.aulast=Strutz&amp;rft.aufirst=T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFRoss2019" class="citation journal cs1">Ross, I.M. (July 2019). <a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cam.2018.12.044">"An optimal control theory for nonlinear optimization"</a>. <i>Journal of Computational and Applied Mathematics</i>. <b>354</b>: <span class="nowrap">39–</span>51. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cam.2018.12.044">10.1016/j.cam.2018.12.044</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:127649426">127649426</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Computational+and+Applied+Mathematics&amp;rft.atitle=An+optimal+control+theory+for+nonlinear+optimization&amp;rft.volume=354&amp;rft.pages=39-51&amp;rft.date=2019-07&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cam.2018.12.044&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A127649426%23id-name%3DS2CID&amp;rft.aulast=Ross&amp;rft.aufirst=I.M.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.cam.2018.12.044&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFNesterov2004" class="citation book cs1"><a href="/wiki/Yurii_Nesterov" title="Yurii Nesterov">Nesterov, Yurii</a> (2004). <i>Introductory Lectures on Convex Optimization&nbsp;: A Basic Course</i>. Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/1-4020-7553-7" title="Special:BookSources/1-4020-7553-7"><bdi>1-4020-7553-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introductory+Lectures+on+Convex+Optimization+%3A+A+Basic+Course&amp;rft.pub=Springer&amp;rft.date=2004&amp;rft.isbn=1-4020-7553-7&amp;rft.aulast=Nesterov&amp;rft.aufirst=Yurii&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFVandenberghe2019" class="citation web cs1">Vandenberghe, Lieven (2019). <a rel="nofollow" class="external text" href="https://www.seas.ucla.edu/~vandenbe/236C/lectures/fgrad.pdf">"Fast Gradient Methods"</a> <span class="cs1-format">(PDF)</span>. <i>Lecture notes for EE236C at UCLA</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Lecture+notes+for+EE236C+at+UCLA&amp;rft.atitle=Fast+Gradient+Methods&amp;rft.date=2019&amp;rft.aulast=Vandenberghe&amp;rft.aufirst=Lieven&amp;rft_id=https%3A%2F%2Fwww.seas.ucla.edu%2F~vandenbe%2F236C%2Flectures%2Ffgrad.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFWalkington2023" class="citation journal cs1">Walkington, Noel J. (2023). <span class="id-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://epubs.siam.org/doi/10.1137/21M1390037">"Nesterov's Method for Convex Optimization"</a></span>. <i>SIAM Review</i>. <b>65</b> (2): <span class="nowrap">539–</span>562. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137%2F21M1390037">10.1137/21M1390037</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" class="external text" href="https://search.worldcat.org/issn/0036-1445">0036-1445</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Review&amp;rft.atitle=Nesterov%27s+Method+for+Convex+Optimization&amp;rft.volume=65&amp;rft.issue=2&amp;rft.pages=539-562&amp;rft.date=2023&amp;rft_id=info%3Adoi%2F10.1137%2F21M1390037&amp;rft.issn=0036-1445&amp;rft.aulast=Walkington&amp;rft.aufirst=Noel+J.&amp;rft_id=https%3A%2F%2Fepubs.siam.org%2Fdoi%2F10.1137%2F21M1390037&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFKimFessler2016" class="citation journal cs1">Kim, D.; Fessler, J. A. (2016). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5067109">"Optimized First-order Methods for Smooth Convex Minimization"</a>. <i><a href="/wiki/Mathematical_Programming" title="Mathematical Programming">Mathematical Programming</a></i>. <b>151</b> (<span class="nowrap">1–</span>2): <span class="nowrap">81–</span>107. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1406.5468">1406.5468</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10107-015-0949-3">10.1007/s10107-015-0949-3</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&nbsp;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5067109">5067109</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/27765996">27765996</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:207055414">207055414</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematical+Programming&amp;rft.atitle=Optimized+First-order+Methods+for+Smooth+Convex+Minimization&amp;rft.volume=151&amp;rft.issue=%3Cspan+class%3D%22nowrap%22%3E1%E2%80%93%3C%2Fspan%3E2&amp;rft.pages=81-107&amp;rft.date=2016&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5067109%23id-name%3DPMC&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A207055414%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2Fs10107-015-0949-3&amp;rft_id=info%3Aarxiv%2F1406.5468&amp;rft_id=info%3Apmid%2F27765996&amp;rft.aulast=Kim&amp;rft.aufirst=D.&amp;rft.au=Fessler%2C+J.+A.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5067109&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFDrori2017" class="citation journal cs1">Drori, Yoel (2017). "The Exact Information-based Complexity of Smooth Convex Minimization". <i>Journal of Complexity</i>. <b>39</b>: <span class="nowrap">1–</span>16. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1606.01424">1606.01424</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.jco.2016.11.001">10.1016/j.jco.2016.11.001</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:205861966">205861966</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Complexity&amp;rft.atitle=The+Exact+Information-based+Complexity+of+Smooth+Convex+Minimization&amp;rft.volume=39&amp;rft.pages=1-16&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1606.01424&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A205861966%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jco.2016.11.001&amp;rft.aulast=Drori&amp;rft.aufirst=Yoel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFQian1999" class="citation journal cs1">Qian, Ning (January 1999). "On the momentum term in gradient descent learning algorithms". <i><a href="/wiki/Neural_Networks_(journal)" title="Neural Networks (journal)">Neural Networks</a></i>. <b>12</b> (1): <span class="nowrap">145–</span>151. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&nbsp;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.5612">10.1.1.57.5612</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2FS0893-6080%2898%2900116-6">10.1016/S0893-6080(98)00116-6</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/12662723">12662723</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:2783597">2783597</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=On+the+momentum+term+in+gradient+descent+learning+algorithms&amp;rft.volume=12&amp;rft.issue=1&amp;rft.pages=145-151&amp;rft.date=1999-01&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.57.5612%23id-name%3DCiteSeerX&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A2783597%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F12662723&amp;rft_id=info%3Adoi%2F10.1016%2FS0893-6080%2898%2900116-6&amp;rft.aulast=Qian&amp;rft.aufirst=Ning&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://www.willamette.edu/~gorr/classes/cs449/momrate.html">"Momentum and Learning Rate Adaptation"</a>. <a href="/wiki/Willamette_University" title="Willamette University">Willamette University</a><span class="reference-accessdate">. Retrieved <span class="nowrap">17 October</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Momentum+and+Learning+Rate+Adaptation&amp;rft.pub=Willamette+University&amp;rft_id=http%3A%2F%2Fwww.willamette.edu%2F~gorr%2Fclasses%2Fcs449%2Fmomrate.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFGeoffrey_HintonNitish_SrivastavaKevin_Swersky" class="citation web cs1"><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a>; Nitish Srivastava; Kevin Swersky. <a rel="nofollow" class="external text" href="https://www.coursera.org/lecture/neural-networks/the-momentum-method-Oya9a">"The momentum method"</a>. <i><a href="/wiki/Coursera" title="Coursera">Coursera</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">2 October</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Coursera&amp;rft.atitle=The+momentum+method&amp;rft.au=Geoffrey+Hinton&amp;rft.au=Nitish+Srivastava&amp;rft.au=Kevin+Swersky&amp;rft_id=https%3A%2F%2Fwww.coursera.org%2Flecture%2Fneural-networks%2Fthe-momentum-method-Oya9a&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span> Part of a lecture series for the <a href="/wiki/Coursera" title="Coursera">Coursera</a> online course <a rel="nofollow" class="external text" href="https://www.coursera.org/learn/neural-networks">Neural Networks for Machine Learning</a> <a rel="nofollow" class="external text" href="https://web.archive.org/web/20161231174321/https://www.coursera.org/learn/neural-networks">Archived</a> 2016-12-31 at the <a href="/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>.</span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFCombettesPesquet2011" class="citation book cs1">Combettes, P. L.; Pesquet, J.-C. (2011). "Proximal splitting methods in signal processing". In Bauschke, H. H.; <a href="/wiki/Regina_S._Burachik" title="Regina S. Burachik">Burachik, R. S.</a>; Combettes, P. L.; Elser, V.; Luke, D. R.; Wolkowicz, H. (eds.). <i>Fixed-Point Algorithms for Inverse Problems in Science and Engineering</i>. New York: Springer. pp.&nbsp;<span class="nowrap">185–</span>212. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/0912.3522">0912.3522</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-1-4419-9568-1" title="Special:BookSources/978-1-4419-9568-1"><bdi>978-1-4419-9568-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Proximal+splitting+methods+in+signal+processing&amp;rft.btitle=Fixed-Point+Algorithms+for+Inverse+Problems+in+Science+and+Engineering&amp;rft.place=New+York&amp;rft.pages=185-212&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft_id=info%3Aarxiv%2F0912.3522&amp;rft.isbn=978-1-4419-9568-1&amp;rft.aulast=Combettes&amp;rft.aufirst=P.+L.&amp;rft.au=Pesquet%2C+J.-C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://tlienart.github.io/posts/2018/10/27-mirror-descent-algorithm/">"Mirror descent algorithm"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Mirror+descent+algorithm&amp;rft_id=https%3A%2F%2Ftlienart.github.io%2Fposts%2F2018%2F10%2F27-mirror-descent-algorithm%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
<li id="cite_note-:1-34"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_34-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_34-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFBubeck2015" class="citation arxiv cs1">Bubeck, Sébastien (2015). "Convex Optimization: Algorithms and Complexity". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1405.4980">1405.4980</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/math.OC">math.OC</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Convex+Optimization%3A+Algorithms+and+Complexity&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1405.4980&amp;rft.aulast=Bubeck&amp;rft.aufirst=S%C3%A9bastien&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></span>
</li>
</ol></div>
<div class="mw-heading mw-heading2"><h2 id="Further_reading">Further reading</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=15" title="Edit section: Further reading"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFBoydVandenberghe2004" class="citation book cs1"><a href="/wiki/Stephen_P._Boyd" title="Stephen P. Boyd">Boyd, Stephen</a>; Vandenberghe, Lieven (2004). <a rel="nofollow" class="external text" href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf#page=471">"Unconstrained Minimization"</a> <span class="cs1-format">(PDF)</span>. <i>Convex Optimization</i>. New York: Cambridge University Press. pp.&nbsp;<span class="nowrap">457–</span>520. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/0-521-83378-7" title="Special:BookSources/0-521-83378-7"><bdi>0-521-83378-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Unconstrained+Minimization&amp;rft.btitle=Convex+Optimization&amp;rft.place=New+York&amp;rft.pages=457-520&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2004&amp;rft.isbn=0-521-83378-7&amp;rft.aulast=Boyd&amp;rft.aufirst=Stephen&amp;rft.au=Vandenberghe%2C+Lieven&amp;rft_id=https%3A%2F%2Fweb.stanford.edu%2F~boyd%2Fcvxbook%2Fbv_cvxbook.pdf%23page%3D471&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFChongŻak2013" class="citation book cs1">Chong, Edwin K. P.; Żak, Stanislaw H. (2013). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=iD5s0iKXHP8C&amp;pg=PA131">"Gradient Methods"</a>. <i>An Introduction to Optimization</i> (Fourth&nbsp;ed.). Hoboken: Wiley. pp.&nbsp;<span class="nowrap">131–</span>160. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-1-118-27901-4" title="Special:BookSources/978-1-118-27901-4"><bdi>978-1-118-27901-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Gradient+Methods&amp;rft.btitle=An+Introduction+to+Optimization&amp;rft.place=Hoboken&amp;rft.pages=131-160&amp;rft.edition=Fourth&amp;rft.pub=Wiley&amp;rft.date=2013&amp;rft.isbn=978-1-118-27901-4&amp;rft.aulast=Chong&amp;rft.aufirst=Edwin+K.+P.&amp;rft.au=%C5%BBak%2C+Stanislaw+H.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DiD5s0iKXHP8C%26pg%3DPA131&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFHimmelblau1972" class="citation book cs1">Himmelblau, David M. (1972). "Unconstrained Minimization Procedures Using Derivatives". <i>Applied Nonlinear Programming</i>. New York: McGraw-Hill. pp.&nbsp;<span class="nowrap">63–</span>132. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/0-07-028921-2" title="Special:BookSources/0-07-028921-2"><bdi>0-07-028921-2</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Unconstrained+Minimization+Procedures+Using+Derivatives&amp;rft.btitle=Applied+Nonlinear+Programming&amp;rft.place=New+York&amp;rft.pages=63-132&amp;rft.pub=McGraw-Hill&amp;rft.date=1972&amp;rft.isbn=0-07-028921-2&amp;rft.aulast=Himmelblau&amp;rft.aufirst=David+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></li></ul>
<div class="mw-heading mw-heading2"><h2 id="External_links">External links</h2><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Gradient_descent&amp;action=edit&amp;section=16" title="Edit section: External links"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>
<style data-mw-deduplicate="TemplateStyles:r1290876196">.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:var(--background-color-interactive-subtle,#f8f9fa);display:flow-root}.mw-parser-output .infobox .side-box{font-size:100%}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1;min-width:0}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}</style><style data-mw-deduplicate="TemplateStyles:r1237033735">@media print{body.ns-0 .mw-parser-output .sistersitebox{display:none!important}}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sistersitebox img[src*="Wiktionary-logo-en-v2.svg"]{background-color:white}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sistersitebox img[src*="Wiktionary-logo-en-v2.svg"]{background-color:white}}</style><div class="side-box side-box-right plainlinks sistersitebox"><style data-mw-deduplicate="TemplateStyles:r1126788409">.mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}</style>
<div class="side-box-flex">
<div class="side-box-image"><span class="noviewer" typeof="mw:File"><a href="/wiki/File:Commons-logo.svg" class="mw-file-description"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/40px-Commons-logo.svg.png" decoding="async" width="30" height="40" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/60px-Commons-logo.svg.png 1.5x" data-file-width="1024" data-file-height="1376"></a></span></div>
<div class="side-box-text plainlist">Wikimedia Commons has media related to <span style="font-weight: bold; font-style: italic;"><a href="https://commons.wikimedia.org/wiki/Category:Gradient_descent" class="extiw" title="commons:Category:Gradient descent">Gradient descent</a></span>.</div></div>
</div>
<ul><li><a rel="nofollow" class="external text" href="https://codingplayground.blogspot.it/2013/05/learning-linear-regression-with.html">Using gradient descent in C++, Boost, Ublas for linear regression</a></li>
<li><a rel="nofollow" class="external text" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient">Series of Khan Academy videos discusses gradient ascent</a></li>
<li><a rel="nofollow" class="external text" href="http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent">Online book teaching gradient descent in deep neural network context</a></li>
<li>Archived at <a rel="nofollow" class="external text" href="https://ghostarchive.org/varchive/youtube/20211211/IHZwWFHWa-w">Ghostarchive</a> and the <a rel="nofollow" class="external text" href="https://web.archive.org/web/20171016173155/https://www.youtube.com/watch?v=IHZwWFHWa-w">Wayback Machine</a>: <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">"Gradient Descent, How Neural Networks Learn"</a>. <i>3Blue1Brown</i>. October 16, 2017 – via <a href="/wiki/YouTube" title="YouTube">YouTube</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=3Blue1Brown&amp;rft.atitle=Gradient+Descent%2C+How+Neural+Networks+Learn&amp;rft.date=2017-10-16&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIHZwWFHWa-w%26list%3DPLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi%26index%3D2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1238218222"><cite id="CITEREFGarrigosGower2023" class="citation arxiv cs1">Garrigos, Guillaume; Gower, Robert M. (2023). "Handbook of Convergence Theorems for (Stochastic) Gradient Methods". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2301.11235">2301.11235</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/math.OC">math.OC</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Handbook+of+Convergence+Theorems+for+%28Stochastic%29+Gradient+Methods&amp;rft.date=2023&amp;rft_id=info%3Aarxiv%2F2301.11235&amp;rft.aulast=Garrigos&amp;rft.aufirst=Guillaume&amp;rft.au=Gower%2C+Robert+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGradient+descent" class="Z3988"></span></li></ul>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1236075235">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role="navigation" class="navbox" aria-labelledby="Artificial_intelligence_(AI)426" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence_navbox" title="Template:Artificial intelligence navbox"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence_navbox" title="Template talk:Artificial intelligence navbox"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence_navbox" title="Special:EditPage/Template:Artificial intelligence navbox"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Artificial_intelligence_(AI)426" style="font-size:114%;margin:0 4em"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a> (AI)</div></th></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a>
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">timeline</a></li></ul></li>
<li><a href="/wiki/List_of_artificial_intelligence_companies" title="List of artificial intelligence companies">Companies</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Parameter" title="Parameter">Parameter</a>
<ul><li><a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">Hyperparameter</a></li></ul></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Double_descent" title="Double descent">Double descent</a></li>
<li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a class="mw-selflink selflink">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li>
<li><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi-Newton method</a></li>
<li><a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">Conjugate gradient method</a></li></ul></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Normalization_(machine_learning)" title="Normalization (machine learning)">Normalization</a>
<ul><li><a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a></li></ul></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Gating_mechanism" title="Gating mechanism">Gating</a></li>
<li><a href="/wiki/Weight_initialization" title="Weight initialization">Weight initialization</a></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_data_sets" title="Training, validation, and test data sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Prompt_engineering" title="Prompt engineering">Prompt engineering</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a>
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Imitation_learning" title="Imitation learning">Imitation</a></li>
<li><a href="/wiki/Policy_gradient_method" title="Policy gradient method">Policy gradient</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Latent_diffusion_model" title="Latent diffusion model">Latent diffusion model</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Retrieval-augmented_generation" title="Retrieval-augmented generation">RAG</a></li>
<li><a href="/wiki/Uncanny_valley" title="Uncanny valley">Uncanny valley</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reflection_(artificial_intelligence)" class="mw-redirect" title="Reflection (artificial intelligence)">Reflection</a></li>
<li><a href="/wiki/Recursive_self-improvement" title="Recursive self-improvement">Recursive self-improvement</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Word_embedding" title="Word embedding">Word embedding</a></li>
<li><a href="/wiki/Vibe_coding" title="Vibe coding">Vibe coding</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Applications</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/Prompt_engineering#In-context_learning" title="Prompt engineering">In-context learning</a></li></ul></li>
<li><a href="/wiki/Neural_network_(machine_learning)" title="Neural network (machine learning)">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large language model</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li></ul></li>
<li><a href="/wiki/Reasoning_language_model" title="Reasoning language model">Reasoning language model</a></li>
<li><a href="/wiki/Model_Context_Protocol" title="Model Context Protocol">Model Context Protocol</a></li>
<li><a href="/wiki/Intelligent_agent" title="Intelligent agent">Intelligent agent</a></li>
<li><a href="/wiki/Artificial_human_companion" title="Artificial human companion">Artificial human companion</a></li>
<li><a href="/wiki/Humanity%27s_Last_Exam" title="Humanity's Last Exam">Humanity's Last Exam</a></li>
<li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence (AGI)</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a>
<ul><li><a href="/wiki/15.ai" title="15.ai">15.ai</a></li>
<li><a href="/wiki/ElevenLabs" title="ElevenLabs">ElevenLabs</a></li></ul></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a>
<ul><li><a href="/wiki/Whisper_(speech_recognition_system)" title="Whisper (speech recognition system)">Whisper</a></li></ul></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/Text-to-image_model" title="Text-to-image model">Text-to-image models</a>
<ul><li><a href="/wiki/Aurora_(text-to-image_model)" class="mw-redirect" title="Aurora (text-to-image model)">Aurora</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Adobe_Firefly" title="Adobe Firefly">Firefly</a></li>
<li><a href="/wiki/Flux_(text-to-image_model)" title="Flux (text-to-image model)">Flux</a></li>
<li><a href="/wiki/Ideogram_(text-to-image_model)" title="Ideogram (text-to-image model)">Ideogram</a></li>
<li><a href="/wiki/Imagen_(text-to-image_model)" title="Imagen (text-to-image model)">Imagen</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Recraft" title="Recraft">Recraft</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul></li>
<li><a href="/wiki/Text-to-video_model" title="Text-to-video model">Text-to-video models</a>
<ul><li><a href="/wiki/Dream_Machine_(text-to-video_model)" title="Dream Machine (text-to-video model)">Dream Machine</a></li>
<li><a href="/wiki/Runway_(company)#Services_and_technologies" title="Runway (company)">Runway Gen</a></li>
<li><a href="/wiki/MiniMax_(company)#Hailuo_AI" title="MiniMax (company)">Hailuo AI</a></li>
<li><a href="/wiki/Kling_(text-to-video_model)" class="mw-redirect" title="Kling (text-to-video model)">Kling</a></li>
<li><a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a></li>
<li><a href="/wiki/Veo_(text-to-video_model)" title="Veo (text-to-video model)">Veo</a></li></ul></li>
<li><a href="/wiki/Music_and_artificial_intelligence" title="Music and artificial intelligence">Music generation</a>
<ul><li><a href="/wiki/Riffusion" title="Riffusion">Riffusion</a></li>
<li><a href="/wiki/Suno_AI" title="Suno AI">Suno AI</a></li>
<li><a href="/wiki/Udio" title="Udio">Udio</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Text</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/T5_(language_model)" title="T5 (language model)">T5</a></li>
<li><a href="/wiki/Llama_(language_model)" title="Llama (language model)">Llama</a></li>
<li><a href="/wiki/Chinchilla_(language_model)" title="Chinchilla (language model)">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/Generative_pre-trained_transformer" title="Generative pre-trained transformer">GPT</a>
<ul><li><a href="/wiki/GPT-1" title="GPT-1">1</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">3</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">J</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">4</a></li>
<li><a href="/wiki/GPT-4o" title="GPT-4o">4o</a></li>
<li><a href="/wiki/OpenAI_o1" title="OpenAI o1">o1</a></li>
<li><a href="/wiki/OpenAI_o3" title="OpenAI o3">o3</a></li>
<li><a href="/wiki/GPT-4.5" title="GPT-4.5">4.5</a></li>
<li><a href="/wiki/GPT-4.1" title="GPT-4.1">4.1</a></li>
<li><a href="/wiki/OpenAI_o4-mini" title="OpenAI o4-mini">o4-mini</a></li></ul></li>
<li><a href="/wiki/Claude_(language_model)" title="Claude (language model)">Claude</a></li>
<li><a href="/wiki/Gemini_(language_model)" title="Gemini (language model)">Gemini</a>
<ul><li><a href="/wiki/Gemini_(chatbot)" title="Gemini (chatbot)">chatbot</a></li></ul></li>
<li><a href="/wiki/Grok_(chatbot)" title="Grok (chatbot)">Grok</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/DBRX" title="DBRX">DBRX</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/IBM_Watsonx" title="IBM Watsonx">IBM Watsonx</a></li>
<li><a href="/wiki/IBM_Granite" title="IBM Granite">Granite</a></li>
<li><a href="/wiki/Huawei_PanGu" title="Huawei PanGu">PanGu-Σ</a></li>
<li><a href="/wiki/DeepSeek_(chatbot)" title="DeepSeek (chatbot)">DeepSeek</a></li>
<li><a href="/wiki/Qwen" title="Qwen">Qwen</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/AutoGPT" title="AutoGPT">AutoGPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a></li>
<li><a href="/wiki/Warren_Sturgis_McCulloch" title="Warren Sturgis McCulloch">Warren Sturgis McCulloch</a></li>
<li><a href="/wiki/Walter_Pitts" title="Walter Pitts">Walter Pitts</a></li>
<li><a href="/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a></li>
<li><a href="/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a></li>
<li><a href="/wiki/Shun%27ichi_Amari" title="Shun'ichi Amari">Shun'ichi Amari</a></li>
<li><a href="/wiki/Kunihiko_Fukushima" title="Kunihiko Fukushima">Kunihiko Fukushima</a></li>
<li><a href="/wiki/Takeo_Kanade" title="Takeo Kanade">Takeo Kanade</a></li>
<li><a href="/wiki/Marvin_Minsky" title="Marvin Minsky">Marvin Minsky</a></li>
<li><a href="/wiki/John_McCarthy_(computer_scientist)" title="John McCarthy (computer scientist)">John McCarthy</a></li>
<li><a href="/wiki/Nathaniel_Rochester_(computer_scientist)" title="Nathaniel Rochester (computer scientist)">Nathaniel Rochester</a></li>
<li><a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a></li>
<li><a href="/wiki/Cliff_Shaw" title="Cliff Shaw">Cliff Shaw</a></li>
<li><a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a></li>
<li><a href="/wiki/Oliver_Selfridge" title="Oliver Selfridge">Oliver Selfridge</a></li>
<li><a href="/wiki/Frank_Rosenblatt" title="Frank Rosenblatt">Frank Rosenblatt</a></li>
<li><a href="/wiki/Bernard_Widrow" title="Bernard Widrow">Bernard Widrow</a></li>
<li><a href="/wiki/Joseph_Weizenbaum" title="Joseph Weizenbaum">Joseph Weizenbaum</a></li>
<li><a href="/wiki/Seymour_Papert" title="Seymour Papert">Seymour Papert</a></li>
<li><a href="/wiki/Seppo_Linnainmaa" title="Seppo Linnainmaa">Seppo Linnainmaa</a></li>
<li><a href="/wiki/Paul_Werbos" title="Paul Werbos">Paul Werbos</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/John_Hopfield" title="John Hopfield">John Hopfield</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Lotfi_A._Zadeh" title="Lotfi A. Zadeh">Lotfi A. Zadeh</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/James_Goodnight" title="James Goodnight">James Goodnight</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Ilya_Sutskever" title="Ilya Sutskever">Ilya Sutskever</a></li>
<li><a href="/wiki/Alex_Krizhevsky" title="Alex Krizhevsky">Alex Krizhevsky</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Andrej_Karpathy" title="Andrej Karpathy">Andrej Karpathy</a></li>
<li><a href="/wiki/Ashish_Vaswani" title="Ashish Vaswani">Ashish Vaswani</a></li>
<li><a href="/wiki/Noam_Shazeer" title="Noam Shazeer">Noam Shazeer</a></li>
<li><a href="/wiki/Aidan_Gomez" title="Aidan Gomez">Aidan Gomez</a></li>
<li><a href="/wiki/Mustafa_Suleyman" title="Mustafa Suleyman">Mustafa Suleyman</a></li>
<li><a href="/wiki/Fran%C3%A7ois_Chollet" title="François Chollet">François Chollet</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(deep_learning_architecture)" title="Transformer (deep learning architecture)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision transformer (ViT)</a></li></ul></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network (CNN)</a></li>
<li><a href="/wiki/Residual_neural_network" title="Residual neural network">Residual neural network (RNN)</a></li>
<li><a href="/wiki/Highway_network" title="Highway network">Highway network</a></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network (GNN)</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/20px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/40px-Symbol_category_class.svg.png 1.5x" data-file-width="180" data-file-height="185"></span></span> <a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Category</a></li></ul>
</div></td></tr></tbody></table></div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1236075235"></div><div role="navigation" class="navbox" aria-labelledby="Optimization:_Algorithms,_methods,_and_heuristics381" style="padding:3px"><table class="nowraplinks hlist mw-collapsible uncollapsed navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="3"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1239400231"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Optimization_algorithms" title="Template:Optimization algorithms"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Optimization_algorithms" title="Template talk:Optimization algorithms"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Optimization_algorithms" title="Special:EditPage/Template:Optimization algorithms"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Optimization:_Algorithms,_methods,_and_heuristics381" style="font-size:114%;margin:0 4em"><a href="/wiki/Mathematical_optimization" title="Mathematical optimization">Optimization</a>: <a href="/wiki/Optimization_algorithm" class="mw-redirect" title="Optimization algorithm">Algorithms</a>, <a href="/wiki/Iterative_method" title="Iterative method">methods</a>, and <a href="/wiki/Heuristic_algorithm" class="mw-redirect" title="Heuristic algorithm">heuristics</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks mw-collapsible uncollapsed navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div id="Unconstrained_nonlinear381" style="font-size:114%;margin:0 4em"><a href="/wiki/Nonlinear_programming" title="Nonlinear programming">Unconstrained nonlinear</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Function_(mathematics)" title="Function (mathematics)">Functions</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Golden-section_search" title="Golden-section search">Golden-section search</a></li>
<li><a href="/wiki/Powell%27s_method" title="Powell's method">Powell's method</a></li>
<li><a href="/wiki/Line_search" title="Line search">Line search</a></li>
<li><a href="/wiki/Nelder%E2%80%93Mead_method" title="Nelder–Mead method">Nelder–Mead method</a></li>
<li><a href="/wiki/Successive_parabolic_interpolation" title="Successive parabolic interpolation">Successive parabolic interpolation</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Gradient" title="Gradient">Gradients</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Local_convergence" title="Local convergence">Convergence</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Trust_region" title="Trust region">Trust region</a></li>
<li><a href="/wiki/Wolfe_conditions" title="Wolfe conditions">Wolfe conditions</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Quasi-Newton_method" title="Quasi-Newton method">Quasi–Newton</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm" title="Berndt–Hall–Hall–Hausman algorithm">Berndt–Hall–Hall–Hausman</a></li>
<li><a href="/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" title="Broyden–Fletcher–Goldfarb–Shanno algorithm">Broyden–Fletcher–Goldfarb–Shanno</a> and <a href="/wiki/Limited-memory_BFGS" title="Limited-memory BFGS">L-BFGS</a></li>
<li><a href="/wiki/Davidon%E2%80%93Fletcher%E2%80%93Powell_formula" title="Davidon–Fletcher–Powell formula">Davidon–Fletcher–Powell</a></li>
<li><a href="/wiki/Symmetric_rank-one" title="Symmetric rank-one">Symmetric rank-one (SR1)</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Iterative_method" title="Iterative method">Other methods</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Nonlinear_conjugate_gradient_method" title="Nonlinear conjugate gradient method">Conjugate gradient</a></li>
<li><a href="/wiki/Gauss%E2%80%93Newton_algorithm" title="Gauss–Newton algorithm">Gauss–Newton</a></li>
<li><a class="mw-selflink selflink">Gradient</a></li>
<li><a href="/wiki/Mirror_descent" title="Mirror descent">Mirror</a></li>
<li><a href="/wiki/Levenberg%E2%80%93Marquardt_algorithm" title="Levenberg–Marquardt algorithm">Levenberg–Marquardt</a></li>
<li><a href="/wiki/Powell%27s_dog_leg_method" title="Powell's dog leg method">Powell's dog leg method</a></li>
<li><a href="/wiki/Truncated_Newton_method" title="Truncated Newton method">Truncated Newton</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Hessian_matrix" title="Hessian matrix">Hessians</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Newton%27s_method_in_optimization" title="Newton's method in optimization">Newton's method</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table><div></div></td><td class="noviewer navbox-image" rowspan="5" style="width:1px;padding:0 0 0 2px"><div><figure class="mw-halign-right" typeof="mw:File"><a href="/wiki/File:Max_paraboloid.svg" class="mw-file-description" title="Optimization computes maxima and minima."><img alt="Graph of a strictly concave quadratic function with unique maximum." src="//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Max_paraboloid.svg/250px-Max_paraboloid.svg.png" decoding="async" width="150" height="120" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/72/Max_paraboloid.svg/330px-Max_paraboloid.svg.png 2x" data-file-width="700" data-file-height="560"></a><figcaption>Optimization computes maxima and minima.</figcaption></figure></div></td></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks mw-collapsible mw-collapsed navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div id="Constrained_nonlinear381" style="font-size:114%;margin:0 4em"><a href="/wiki/Nonlinear_programming" title="Nonlinear programming">Constrained nonlinear</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">General</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Barrier_function" title="Barrier function">Barrier methods</a></li>
<li><a href="/wiki/Penalty_method" title="Penalty method">Penalty methods</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Differentiable</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Augmented_Lagrangian_method" title="Augmented Lagrangian method">Augmented Lagrangian methods</a></li>
<li><a href="/wiki/Sequential_quadratic_programming" title="Sequential quadratic programming">Sequential quadratic programming</a></li>
<li><a href="/wiki/Successive_linear_programming" title="Successive linear programming">Successive linear programming</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table><div></div></td></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks mw-collapsible mw-collapsed navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div id="Convex_optimization381" style="font-size:114%;margin:0 4em"><a href="/wiki/Convex_optimization" title="Convex optimization">Convex optimization</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Convex_minimization" class="mw-redirect" title="Convex minimization">Convex<br> minimization</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Cutting-plane_method" title="Cutting-plane method">Cutting-plane method</a></li>
<li><a href="/wiki/Frank%E2%80%93Wolfe_algorithm" title="Frank–Wolfe algorithm">Reduced gradient (Frank–Wolfe)</a></li>
<li><a href="/wiki/Subgradient_method" title="Subgradient method">Subgradient method</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Linear_programming" title="Linear programming">Linear</a> and<br><a href="/wiki/Quadratic_programming" title="Quadratic programming">quadratic</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Linear_programming#Interior_point" title="Linear programming">Interior point</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Affine_scaling" title="Affine scaling">Affine scaling</a></li>
<li><a href="/wiki/Ellipsoid_method" title="Ellipsoid method">Ellipsoid algorithm of Khachiyan</a></li>
<li><a href="/wiki/Karmarkar%27s_algorithm" title="Karmarkar's algorithm">Projective algorithm of Karmarkar</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Matroid" title="Matroid">Basis-</a><a href="/wiki/Exchange_algorithm" class="mw-redirect" title="Exchange algorithm">exchange</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Simplex_algorithm" title="Simplex algorithm">Simplex algorithm of Dantzig</a></li>
<li><a href="/wiki/Revised_simplex_method" title="Revised simplex method">Revised simplex algorithm</a></li>
<li><a href="/wiki/Criss-cross_algorithm" title="Criss-cross algorithm">Criss-cross algorithm</a></li>
<li><a href="/wiki/Lemke%27s_algorithm" title="Lemke's algorithm">Principal pivoting algorithm of Lemke</a></li>
<li><a href="/wiki/Active-set_method" title="Active-set method">Active-set method</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table><div></div></td></tr></tbody></table><div></div></td></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks mw-collapsible mw-collapsed navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div id="Combinatorial381" style="font-size:114%;margin:0 4em"><a href="/wiki/Combinatorial_optimization" title="Combinatorial optimization">Combinatorial</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Paradigms</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Approximation_algorithm" title="Approximation algorithm">Approximation algorithm</a></li>
<li><a href="/wiki/Dynamic_programming" title="Dynamic programming">Dynamic programming</a></li>
<li><a href="/wiki/Greedy_algorithm" title="Greedy algorithm">Greedy algorithm</a></li>
<li><a href="/wiki/Integer_programming" title="Integer programming">Integer programming</a>
<ul><li><a href="/wiki/Branch_and_bound" title="Branch and bound">Branch and bound</a>/<a href="/wiki/Branch_and_cut" title="Branch and cut">cut</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Graph_algorithm" class="mw-redirect" title="Graph algorithm">Graph<br> algorithms</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th id="Minimum_spanning_tree52" scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Minimum_spanning_tree" title="Minimum spanning tree">Minimum<br> spanning tree</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Bor%C5%AFvka%27s_algorithm" title="Borůvka's algorithm">Borůvka</a></li>
<li><a href="/wiki/Prim%27s_algorithm" title="Prim's algorithm">Prim</a></li>
<li><a href="/wiki/Kruskal%27s_algorithm" title="Kruskal's algorithm">Kruskal</a></li></ul>
</div></td></tr></tbody></table><div>
    </div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th id="Shortest_path39" scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Shortest_path_problem" title="Shortest path problem">Shortest path</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Bellman%E2%80%93Ford_algorithm" title="Bellman–Ford algorithm">Bellman–Ford</a>
<ul><li><a href="/wiki/Shortest_Path_Faster_Algorithm" class="mw-redirect" title="Shortest Path Faster Algorithm">SPFA</a></li></ul></li>
<li><a href="/wiki/Dijkstra%27s_algorithm" title="Dijkstra's algorithm">Dijkstra</a></li>
<li><a href="/wiki/Floyd%E2%80%93Warshall_algorithm" title="Floyd–Warshall algorithm">Floyd–Warshall</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Flow_network" title="Flow network">Network flows</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Dinic%27s_algorithm" title="Dinic's algorithm">Dinic</a></li>
<li><a href="/wiki/Edmonds%E2%80%93Karp_algorithm" title="Edmonds–Karp algorithm">Edmonds–Karp</a></li>
<li><a href="/wiki/Ford%E2%80%93Fulkerson_algorithm" title="Ford–Fulkerson algorithm">Ford–Fulkerson</a></li>
<li><a href="/wiki/Push%E2%80%93relabel_maximum_flow_algorithm" title="Push–relabel maximum flow algorithm">Push–relabel maximum flow</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table><div></div></td></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks mw-collapsible mw-collapsed navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div id="Metaheuristics381" style="font-size:114%;margin:0 4em"><a href="/wiki/Metaheuristic" title="Metaheuristic">Metaheuristics</a></div></th></tr><tr><td colspan="2" class="navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithm</a></li>
<li><a href="/wiki/Hill_climbing" title="Hill climbing">Hill climbing</a></li>
<li><a href="/wiki/Local_search_(optimization)" title="Local search (optimization)">Local search</a></li>
<li><a href="/wiki/Parallel_metaheuristic" title="Parallel metaheuristic">Parallel metaheuristics</a></li>
<li><a href="/wiki/Simulated_annealing" title="Simulated annealing">Simulated annealing</a></li>
<li><a href="/wiki/Spiral_optimization_algorithm" title="Spiral optimization algorithm">Spiral optimization algorithm</a></li>
<li><a href="/wiki/Tabu_search" title="Tabu search">Tabu search</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><td class="navbox-abovebelow" colspan="3"><div>
<ul><li><a href="/wiki/Comparison_of_optimization_software" title="Comparison of optimization software">Software</a></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐7b48b5fb74‐xz647
Cached time: 20250803162030
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.963 seconds
Real time usage: 1.295 seconds
Preprocessor visited node count: 3933/1000000
Revision size: 40302/2097152 bytes
Post‐expand include size: 203054/2097152 bytes
Template argument size: 3072/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 169238/5000000 bytes
Lua time usage: 0.539/10.000 seconds
Lua memory usage: 6171474/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  840.648      1 -total
 45.01%  378.377      1 Template:Reflist
 23.28%  195.703     14 Template:Cite_book
 14.18%  119.200     15 Template:Cite_journal
 13.66%  114.832      1 Template:Machine_learning
 13.36%  112.349      1 Template:Sidebar_with_collapsible_lists
 13.25%  111.381      1 Template:Short_description
  8.14%   68.462      2 Template:Pagetype
  6.87%   57.764     10 Template:Navbox
  5.27%   44.283      1 Template:Artificial_intelligence_navbox
-->

<!-- Saved in parser cache with key enwiki:pcache:201489:|#|:idhash:canonical and timestamp 20250803162030 and revision id 1300672032. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> --><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1&amp;usesul3=1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Gradient_descent&amp;oldid=1300672032">https://en.wikipedia.org/w/index.php?title=Gradient_descent&amp;oldid=1300672032</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Mathematical_optimization" title="Category:Mathematical optimization">Mathematical optimization</a></li><li><a href="/wiki/Category:First_order_methods" title="Category:First order methods">First order methods</a></li><li><a href="/wiki/Category:Optimization_algorithms_and_methods" title="Category:Optimization algorithms and methods">Optimization algorithms and methods</a></li><li><a href="/wiki/Category:Gradient_methods" title="Category:Gradient methods">Gradient methods</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1:Vancouver_names_with_accept_markup" title="Category:CS1:Vancouver names with accept markup">CS1:Vancouver names with accept markup</a></li><li><a href="/wiki/Category:Webarchive_template_wayback_links" title="Category:Webarchive template wayback links">Webarchive template wayback links</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Commons_category_link_is_on_Wikidata" title="Category:Commons category link is on Wikidata">Commons category link is on Wikidata</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 15 July 2025, at 19:08<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a href="/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a>;
additional terms may apply. By using this site, you agree to the <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use" class="extiw" title="foundation:Special:MyLanguage/Policy:Terms of Use">Terms of Use</a> and <a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy" class="extiw" title="foundation:Special:MyLanguage/Policy:Privacy policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a rel="nofollow" class="external text" href="https://wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Gradient_descent&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://www.wikimedia.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/static/images/footer/wikimedia-button.svg" width="84" height="29"><img src="/static/images/footer/wikimedia.svg" width="25" height="25" alt="Wikimedia Foundation" lang="en" loading="lazy"></picture></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/" class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled"><picture><source media="(min-width: 500px)" srcset="/w/resources/assets/poweredby_mediawiki.svg" width="88" height="31"><img src="/w/resources/assets/mediawiki_compact.svg" alt="Powered by MediaWiki" lang="en" width="25" height="25" loading="lazy"></picture></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-header-container vector-sticky-header-container no-font-mode-scale">
	<div id="vector-sticky-header" class="vector-sticky-header">
		<div class="vector-sticky-header-start">
			<div class="vector-sticky-header-icon-start vector-button-flush-left vector-button-flush-right" aria-hidden="true">
				<button class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle" tabindex="-1" data-event-name="ui.vector-sticky-search-form.icon"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
			</button>
		</div>
			
		<div role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box">
			<div class="vector-typeahead-search-container">
				<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail">
					<form action="/w/index.php" id="vector-sticky-search-form" class="cdx-search-input cdx-search-input--has-end-button">
						<div  class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
							<div class="cdx-text-input cdx-text-input--has-start-icon">
								<input
									class="cdx-text-input__input mw-searchInput" autocomplete="off"
									
									type="search" name="search" placeholder="Search Wikipedia">
								<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
							</div>
							<input type="hidden" name="title" value="Special:Search">
						</div>
						<button class="cdx-button cdx-search-input__end-button">Search</button>
					</form>
				</div>
			</div>
		</div>
		<div class="vector-sticky-header-context-bar">
				<nav aria-label="Contents" class="vector-toc-landmark">
						
					<div id="vector-sticky-header-toc" class="vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left"  >
						<input type="checkbox" id="vector-sticky-header-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-sticky-header-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
						<label id="vector-sticky-header-toc-label" for="vector-sticky-header-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
						</label>
						<div class="vector-dropdown-content">
					
						<div id="vector-sticky-header-toc-unpinned-container" class="vector-unpinned-container">
						</div>
					
						</div>
					</div>
			</nav>
				<div class="vector-sticky-header-context-bar-primary" aria-hidden="true" ><span class="mw-page-title-main">Gradient descent</span></div>
			</div>
		</div>
		<div class="vector-sticky-header-end" aria-hidden="true">
			<div class="vector-sticky-header-icons">
				<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-talk-sticky-header" tabindex="-1" data-event-name="talk-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-subject-sticky-header" tabindex="-1" data-event-name="subject-sticky-header"><span class="vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-history-sticky-header" tabindex="-1" data-event-name="history-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink" id="ca-watchstar-sticky-header" tabindex="-1" data-event-name="watch-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-edit-sticky-header" tabindex="-1" data-event-name="wikitext-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-ve-edit-sticky-header" tabindex="-1" data-event-name="ve-edit-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit"></span>

<span></span>
			</a>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only" id="ca-viewsource-sticky-header" tabindex="-1" data-event-name="ve-edit-protected-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock"></span>

<span></span>
			</a>
		</div>
			<div class="vector-sticky-header-buttons">
				<button class="cdx-button cdx-button--weight-quiet mw-interlanguage-selector" id="p-lang-btn-sticky-header" tabindex="-1" data-event-name="ui.dropdown-p-lang-btn-sticky-header"><span class="vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language"></span>

<span>26 languages</span>
			</button>
			<a href="#" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive" id="ca-addsection-sticky-header" tabindex="-1" data-event-name="addsection-sticky-header"><span class="vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive"></span>

<span>Add topic</span>
			</a>
		</div>
			<div class="vector-sticky-header-icon-end">
				<div class="vector-user-links">
				</div>
			</div>
		</div>
	</div>
</div>
<div class="mw-portlet mw-portlet-dock-bottom emptyPortlet" id="p-dock-bottom">
	<ul>
		
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.eqiad.main-7b48b5fb74-jzf96","wgBackendResponseTime":296,"wgPageParseReport":{"limitreport":{"cputime":"0.963","walltime":"1.295","ppvisitednodes":{"value":3933,"limit":1000000},"revisionsize":{"value":40302,"limit":2097152},"postexpandincludesize":{"value":203054,"limit":2097152},"templateargumentsize":{"value":3072,"limit":2097152},"expansiondepth":{"value":16,"limit":100},"expensivefunctioncount":{"value":2,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":169238,"limit":5000000},"entityaccesscount":{"value":0,"limit":500},"timingprofile":["100.00%  840.648      1 -total"," 45.01%  378.377      1 Template:Reflist"," 23.28%  195.703     14 Template:Cite_book"," 14.18%  119.200     15 Template:Cite_journal"," 13.66%  114.832      1 Template:Machine_learning"," 13.36%  112.349      1 Template:Sidebar_with_collapsible_lists"," 13.25%  111.381      1 Template:Short_description","  8.14%   68.462      2 Template:Pagetype","  6.87%   57.764     10 Template:Navbox","  5.27%   44.283      1 Template:Artificial_intelligence_navbox"]},"scribunto":{"limitreport-timeusage":{"value":"0.539","limit":"10.000"},"limitreport-memusage":{"value":6171474,"limit":52428800}},"cachereport":{"origin":"mw-web.eqiad.main-7b48b5fb74-xz647","timestamp":"20250803162030","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Gradient descent","url":"https:\/\/en.wikipedia.org\/wiki\/Gradient_descent","sameAs":"http:\/\/www.wikidata.org\/entity\/Q1199743","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q1199743","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2003-03-26T01:29:07Z","dateModified":"2025-07-15T19:08:48Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/4\/4c\/Gradient_Descent_in_2D.webm","headline":"optimization algorithm"}</script>
</body>
</html>